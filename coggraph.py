# coggraph.py
import uuid
from CogUnit import CogUnit
import torch
import random
from env import GridEnvironment
import torch.nn.functional as F



MAX_CONNECTIONS = 4  # æ¯ä¸ªå•å…ƒæœ€å¤šè¿æ¥ 4 ä¸ªä¸‹æ¸¸
N_STATE_CHANNELS = 3
N_GOAL_CHANNELS = 1
INPUT_CHANNELS = N_STATE_CHANNELS + N_GOAL_CHANNELS





class TaskInjector:
    def __init__(self, target_position):
        self.target_position = target_position  # ç›®æ ‡åæ ‡ (x, y)

    def encode_goal(self, env_size):
        """å°†ç›®æ ‡ä½ç½®ç¼–ç æˆ one-hot å‘é‡ï¼ˆä¸è¾“å…¥åŒç»´åº¦ï¼‰"""
        index = self.target_position[1] * env_size + self.target_position[0]
        vec = torch.zeros(env_size * env_size)
        vec[index] = 1.0
        return vec

    def evaluate(self, env, emitter_outputs):
        """è¯„ä¼° emitter æ˜¯å¦â€œæŒ‡å‘â€ç›®æ ‡ä½ç½®"""
        if emitter_outputs is None:
            return False

        pred_index = torch.argmax(emitter_outputs.mean(dim=0)).item()
        x, y = pred_index % env.size, pred_index // env.size
        return (x, y) == self.target_position



# ç†æƒ³æ¯”ä¾‹  emitter : processor : sensor = 1 : 2 : 1
IDEAL_RATIO = {"emitter": 1, "processor": 2, "sensor": 1}
DENOM = sum(IDEAL_RATIO.values())      # =4

# æ¯è½®å…è®¸è½¬æ¢çš„æœ€é«˜æ¯”ä¾‹ï¼ˆ15%ï¼‰
MAX_CONV_FRAC = 0.15

# Î” å®¹å·®ç³»æ•°ï¼šéœ€è¦è‡³å°‘ diff â‰¥ ceil(TOL_FRAC*total) æ‰è§¦å‘
TOL_FRAC = 0.05      # å°è§„æ¨¡æ—¶è‡ªåŠ¨é€€åŒ–æˆ 1


class CogGraph:
    """
    CogGraph ç®¡ç†æ‰€æœ‰ CogUnit çš„é›†åˆå’Œè¿æ¥å…³ç³»ï¼š
    - æ·»åŠ  / åˆ é™¤å•å…ƒ
    - ç®¡ç†è¿æ¥ï¼ˆå¯æ‹“å±•ä¸ºå›¾ï¼‰
    - è°ƒåº¦æ¯ä¸€è½®æ‰€æœ‰ CogUnit çš„æ›´æ–°ã€åˆ†è£‚ã€æ­»äº¡ï¼Œå¹¶ä¼ é€’è¾“å‡º
    """
    def __init__(self):

        self.memory_pool = []  # å­˜æ”¾æ­»äº¡ç»†èƒçš„ gene + last_output + bias info
        self.env_size = 5  # åˆå§‹ç¯å¢ƒ 5x5
        self.env = GridEnvironment(size=self.env_size)  # åˆ›å»ºç¯å¢ƒ
        self.task = TaskInjector(target_position=(self.env_size - 1, self.env_size - 1))  # åˆå§‹ç›®æ ‡ç‚¹
        self.target_vector = self.task.encode_goal(self.env_size)  # åˆå§‹ç›®æ ‡å‘é‡
        self.max_total_energy = 300  # åˆå§‹æœ€å¤§æ€»èƒ½é‡
        self.target_vector = self.task.encode_goal(self.env_size)
        self.connection_usage = {}  # {(from_id, to_id): last_used_step}
        self.current_step = 0
        self.units = []
        self.connections = {}  # {from_id: {to_id: strength_float}}
        self.unit_map = {}     # {unit_id: CogUnit å®ä¾‹} å¿«é€Ÿç´¢å¼•å•å…ƒ

    def add_unit(self, unit: CogUnit):
        # å°†å•å…ƒåŠ å…¥å›¾ç»“æ„ä¸­
        self.units.append(unit)
        self.unit_map[unit.id] = unit
        self.connections[unit.id] = {}

    def remove_unit(self, unit: CogUnit):

        if unit.id not in self.unit_map:
            return  # å·²ç»è¢«åˆ é™¤
        if hasattr(self, "memory_pool") and unit.age > 30:
            self.memory_pool.append({
                "gene": unit.gene.copy(),
                "output": unit.last_output.clone(),
                "role": unit.role,
                "hidden_size": unit.hidden_size
            })

            print(f"[è®°å¿†æ± ] {unit.id} æ­»äº¡ï¼Œé—äº§è®°å½•å·²ä¿å­˜ï¼ˆå…± {len(self.memory_pool)} æ¡ï¼‰")

        # ä»å›¾ä¸­ç§»é™¤å•å…ƒåŠå…¶è¿æ¥
        self.units = [u for u in self.units if u.id != unit.id]
        if unit.id in self.connections:
            del self.connections[unit.id]
        if unit.id in self.unit_map:
            del self.unit_map[unit.id]
        for k in self.connections:
            if unit.id in self.connections[k]:
                del self.connections[k][unit.id]

        # ğŸª¢ é™åˆ¶è®°å¿†æ± å¤§å°
        if len(self.memory_pool) > 200:
            removed = self.memory_pool.pop(0)
            print(f"[è®°å¿†æ± ç»´æŠ¤] è¶…å‡ºå®¹é‡ï¼Œç§»é™¤æœ€æ—§é—äº§ï¼ˆrole={removed['role']}ï¼‰")

    def connect(self, from_unit: CogUnit, to_unit: CogUnit):
        if from_unit.id not in self.connections:
            self.connections[from_unit.id] = {}  # to_id â†’ strength

        if to_unit.id in self.connections[from_unit.id]:
            return

        # è¶…è¿‡ä¸Šé™æ—¶ï¼Œç§»é™¤ strength æœ€å¼±çš„è¿æ¥
        if len(self.connections[from_unit.id]) >= MAX_CONNECTIONS:
            weakest_id = min(
                self.connections[from_unit.id],
                key=lambda uid: self.connections[from_unit.id][uid]
            )
            del self.connections[from_unit.id][weakest_id]
            print(f"[è¿æ¥æ›¿æ¢] {from_unit.id} ç§»é™¤æœ€å¼±è¿æ¥ {weakest_id}")

        # å»ºç«‹æ–°è¿æ¥ï¼Œåˆå§‹æƒé‡ä¸º 1.0
        self.connections[from_unit.id][to_unit.id] = 1.0
        strength = self.connections[from_unit.id][to_unit.id]
        print(f"[è¿æ¥å»ºç«‹] {from_unit.id} â†’ {to_unit.id} (strength={strength:.2f})")

    def total_energy(self):
        return sum(unit.energy for unit in self.units)

    # ========== ç»´åº¦é€‚é…è¾…åŠ© ==========
    def _goal_dim(self) -> int:
        """è¿”å›å½“å‰ç›®æ ‡å‘é‡é•¿åº¦ (= env_sizeÂ²)"""
        return self.env_size * self.env_size

    def _align_to_goal_dim(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        æŠŠä»»æ„é•¿åº¦çš„å‘é‡å¯¹é½åˆ° env_sizeÂ²ï¼š
        - å¦‚æœæ°å¥½ç›¸ç­‰ â†’ åŸæ ·
        - å¦‚æœèƒ½æ•´é™¤ â†’ reshape(k, goal_dim) åæ±‚å‡å€¼ â†’ goal_dim
          ï¼ˆé»˜è®¤ 4 é€šé“æ—¶ç›¸å½“äºæŠŠ 4 ä¸ªé€šé“å‹ç¼©æˆ 1 é€šé“ï¼‰
        - å¦‚æœæ›´é•¿ä½†æ— æ³•æ•´é™¤ â†’ æˆªæ–­åˆ°å‰ goal_dim
        - å¦‚æœæ›´çŸ­ â†’ å³ä¾§è¡¥é›¶
        """
        goal_dim = self._goal_dim()
        length = tensor.shape[-1]

        if length == goal_dim:
            return tensor

        if length % goal_dim == 0:
            k = length // goal_dim
            return tensor.reshape(-1, k, goal_dim).mean(dim=1).squeeze(0)

        if length > goal_dim:            # æˆªæ–­
            return tensor[..., :goal_dim]

        # length < goal_dim  â†’ å³è¡¥é›¶
        pad = (0, goal_dim - length)
        return torch.nn.functional.pad(tensor, pad)


    def merge_redundant_units(self):
        merged_pairs = set()
        new_units = []

        for i, u1 in enumerate(self.units):
            for j in range(i + 1, len(self.units)):
                u2 = self.units[j]

                # è·³è¿‡å·²æ ‡è®°
                if u1.id in merged_pairs or u2.id in merged_pairs:
                    continue

                # å¿…é¡»æ˜¯ processor æˆ– emitter
                if u1.get_role() != u2.get_role():
                    continue

                # è·ç¦»åˆ¤æ–­
                def euclidean(p1, p2):
                    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5

                dist = euclidean(u1.get_position(), u2.get_position())
                if dist > 3.0:
                    continue

                # è¾“å‡ºç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆcosine similarityï¼‰
                # === è¾“å‡ºç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆcosine similarityï¼‰===
                output1 = u1.get_output().squeeze(0)
                output2 = u2.get_output().squeeze(0)

                # ğŸ”¥ è‡ªåŠ¨è¡¥é›¶åˆ°å½“å‰ç¯å¢ƒé¢„æœŸå°ºå¯¸
                target_dim = max(output1.shape[0], output2.shape[0])

                if output1.shape[0] < target_dim:
                    padding = (0, target_dim - output1.shape[0])
                    output1 = torch.nn.functional.pad(output1, padding, value=0)

                if output2.shape[0] < target_dim:
                    padding = (0, target_dim - output2.shape[0])
                    output2 = torch.nn.functional.pad(output2, padding, value=0)

                sim = F.cosine_similarity(output1, output2, dim=0).item()

                if sim < 0.95:
                    continue

                # âœ… æ»¡è¶³æ¡ä»¶ï¼Œæ‰§è¡Œåˆå¹¶
                print(f"[åˆå¹¶è§¦å‘] {u1.id} å’Œ {u2.id} åˆå¹¶ä¸ºæ–°å•å…ƒ")

                merged = CogUnit(role=u1.get_role())
                merged.position = (
                    (u1.get_position()[0] + u2.get_position()[0]) // 2,
                    (u1.get_position()[1] + u2.get_position()[1]) // 2
                )
                merged.state = (u1.state + u2.state) / 2
                merged.age = int((u1.age + u2.age) / 2)
                merged.energy = u1.energy + u2.energy + 0.1  # å¥–åŠ±åˆå¹¶èƒ½é‡
                merged.last_output = (u1.last_output + u2.last_output) / 2

                # åŠ å…¥æ–°å•å…ƒ
                new_units.append(merged)
                merged_pairs.update({u1.id, u2.id})

                # é‡å®šå‘è¿æ¥ï¼š
                for from_id, to_dict in self.connections.items():
                    if u1.id in to_dict or u2.id in to_dict:
                        if from_id in self.unit_map:  # é˜²æ­¢ä¸Šæ¸¸å·²ç»è¢«åˆå¹¶åˆ é™¤
                            self.connect(self.unit_map[from_id], merged)

                for to_id in self.connections.get(u1.id, {}):
                    if to_id in self.unit_map:  # âœ… é˜²æ­¢è¿æ¥åˆ°å·²è¢«åˆ é™¤çš„å•å…ƒ
                        self.connect(merged, self.unit_map[to_id])
                        print(f"[è¿æ¥é‡å®šå‘] {merged.id} â†’ {to_id}ï¼ˆç»§æ‰¿è‡ª {u1.id}ï¼‰")

                for to_id in self.connections.get(u2.id, {}):
                    if to_id in self.unit_map:
                        self.connect(merged, self.unit_map[to_id])
                        print(f"[è¿æ¥é‡å®šå‘] {merged.id} â†’ {to_id}ï¼ˆç»§æ‰¿è‡ª {u2.id}ï¼‰")

        # æ‰§è¡Œåˆ é™¤ & æ·»åŠ 
        for uid in merged_pairs:
            if uid in self.unit_map:
                print(f"[åˆå¹¶åˆ é™¤] {uid}")
                self.remove_unit(self.unit_map[uid])

        for u in new_units:
            self.add_unit(u)

    def restructure_common_subgraphs(self):
        """
        æ£€æŸ¥å¹¶é‡æ„é«˜åº¦å…±ç°ã€è¾“å‡ºç›¸ä¼¼çš„ processor â†’ emitter å­å›¾ç»“æ„ã€‚
        å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªæ–°å­å›¾ï¼šnew_processor â†’ new_emitter
        """
        candidates = []

        # éå†æ‰€æœ‰ processor â†’ emitter è¿æ¥
        for u1 in self.units:
            if u1.get_role() != "processor":
                continue
            for eid in self.connections.get(u1.id, {}):
                if eid not in self.unit_map:
                    continue
                u2 = self.unit_map[eid]
                if u2.get_role() != "emitter":
                    continue
                candidates.append((u1, u2))

        # æ£€æŸ¥æ¯å¯¹å­å›¾æ˜¯å¦æ»¡è¶³å…±ç°ä¸è¾“å‡ºç›¸ä¼¼
        for i in range(len(candidates)):
            for j in range(i + 1, len(candidates)):
                p1, e1 = candidates[i]
                p2, e2 = candidates[j]

                if p1.id == p2.id or e1.id == e2.id:
                    continue

                # 1. æ£€æŸ¥å…±ç°ï¼ˆæœ€è¿‘ 5 æ­¥è°ƒç”¨é¢‘ç‡éƒ½ä¸ä¸º 0ï¼‰
                if min(p1.call_history[-3:], default=0) == 0 or min(p2.call_history[-3:], default=0) == 0:
                    continue

                # 2. æ£€æŸ¥è¾“å‡ºç›¸ä¼¼æ€§
                import torch.nn.functional as F
                out1 = p1.get_output()
                out2 = p2.get_output()

                # ğŸ”¥ è‡ªåŠ¨è¡¥é›¶åˆ°å½“å‰ç¯å¢ƒ target_dim
                target_dim = self.env_size * self.env_size * INPUT_CHANNELS

                if out1.shape[-1] < target_dim:
                    padding = (0, target_dim - out1.shape[-1])
                    out1 = torch.nn.functional.pad(out1, padding, value=0)

                if out2.shape[-1] < target_dim:
                    padding = (0, target_dim - out2.shape[-1])
                    out2 = torch.nn.functional.pad(out2, padding, value=0)

                sim_p = F.cosine_similarity(out1, out2, dim=-1).item()

                # ç»Ÿä¸€è®¡ç®— processor è¾“å‡ºä¸ emitter è¾“å‡ºçš„ç›¸ä¼¼æ€§ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´
                out1 = p1.get_output()
                out2 = p2.get_output()
                out_e1 = e1.get_output()
                out_e2 = e2.get_output()

                max_dim = max(
                    out1.shape[-1], out2.shape[-1],
                    out_e1.shape[-1], out_e2.shape[-1],
                    self.env_size * self.env_size * INPUT_CHANNELS
                )

                def pad_to(tensor, target_dim):
                    if tensor.shape[-1] < target_dim:
                        padding = (0, target_dim - tensor.shape[-1])
                        return F.pad(tensor, padding, value=0)
                    return tensor

                out1 = pad_to(out1, max_dim)
                out2 = pad_to(out2, max_dim)
                out_e1 = pad_to(out_e1, max_dim)
                out_e2 = pad_to(out_e2, max_dim)

                sim_p = F.cosine_similarity(out1, out2, dim=-1).item()
                sim_e = F.cosine_similarity(out_e1, out_e2, dim=-1).item()

                if sim_p > 0.95 and sim_e > 0.95:
                    # âœ… æ»¡è¶³é‡æ„æ¡ä»¶
                    print(f"[é‡æ„è§¦å‘] å­å›¾ ({p1.id}â†’{e1.id}) ä¸ ({p2.id}â†’{e2.id}) ç›¸ä¼¼ï¼Œå¼€å§‹é‡æ„")

                    # åˆ›å»ºæ–°å•å…ƒ
                    new_p = CogUnit(role="processor")
                    new_e = CogUnit(role="emitter")
                    new_p.state = (p1.state + p2.state) / 2
                    new_p.last_output = (p1.last_output + p2.last_output) / 2
                    new_e.state = (e1.state + e2.state) / 2
                    new_e.last_output = (e1.last_output + e2.last_output) / 2

                    new_p.energy = p1.energy + p2.energy + 0.1
                    new_e.energy = e1.energy + e2.energy + 0.1

                    # æ’å…¥æ–°å•å…ƒ
                    self.add_unit(new_p)
                    self.add_unit(new_e)
                    self.connect(new_p, new_e)

                    # å°†æ‰€æœ‰è¿æ¥åˆ° p1 / p2 çš„ä¸Šæ¸¸æŒ‡å‘ new_p
                    for uid in list(self.unit_map):
                        if p1.id in self.connections.get(uid, {}) or p2.id in self.connections.get(uid, {}):
                            self.connect(self.unit_map[uid], new_p)

                    # åˆ é™¤åŸå­å›¾
                    print(f"[é‡æ„åˆ é™¤] åˆ é™¤åŸå­å›¾ ({p1.id}â†’{e1.id}) å’Œ ({p2.id}â†’{e2.id})")
                    self.remove_unit(p1)
                    self.remove_unit(p2)
                    self.remove_unit(e1)
                    self.remove_unit(e2)

                    return  # æ¯è½®åªé‡æ„ä¸€ç»„ï¼Œé¿å…å†²çª

    def assign_subsystems(self, min_size=3, max_size=10):
        """
        è‡ªåŠ¨å‘ç°å±€éƒ¨é«˜å¯†åº¦è¿æ¥åŒºåŸŸï¼Œæ ‡è®°ä¸ºå­ç³»ç»Ÿ
        """
        visited = set()
        subsystem_count = 0

        for unit in self.units:
            if unit.id in visited:
                continue

            # ä»¥å½“å‰unitä¸ºèµ·ç‚¹ï¼Œåšå±€éƒ¨DFS
            cluster = self._dfs_collect_cluster(unit, max_depth=2)

            if min_size <= len(cluster) <= max_size:
                subsystem_id = f"subsys-{subsystem_count}"
                for u in cluster:
                    u.subsystem_id = subsystem_id
                subsystem_count += 1
                print(f"[å­ç³»ç»Ÿç”Ÿæˆ] æ–°å­ç³»ç»Ÿ {subsystem_id}ï¼ŒåŒ…å« {len(cluster)} ä¸ªå•å…ƒ")
                visited.update(u.id for u in cluster)

    def _dfs_collect_cluster(self, start_unit, max_depth=2):
        """
        è¾…åŠ©ï¼šæ·±åº¦ä¼˜å…ˆæœç´¢ï¼Œæ‰¾å‡ºå±€éƒ¨è¿æ¥çš„å•å…ƒç¾¤
        """
        cluster = set()
        stack = [(start_unit, 0)]
        while stack:
            unit, depth = stack.pop()
            if depth > max_depth or unit in cluster:
                continue
            cluster.add(unit)
            for neighbor_id in self.connections.get(unit.id, {}):
                neighbor = self.unit_map.get(neighbor_id)
                if neighbor:
                    stack.append((neighbor, depth + 1))

        return list(cluster)

    def prune_connections(self, prune_ratio=0.2, strengthen_ratio=1.5):
        """
        è‡ªåŠ¨å‰ªæ‰ä½æ•ˆè¿æ¥ï¼Œå¼ºåŒ–é«˜æ•ˆè¿æ¥
        :param prune_ratio: å°äºå…¨å±€å¹³å‡è°ƒç”¨é¢‘ç‡ * prune_ratio çš„è¿æ¥ä¼šè¢«å‰ªæ‰
        :param strengthen_ratio: å¤§äºå…¨å±€å¹³å‡è°ƒç”¨é¢‘ç‡ * strengthen_ratio çš„è¿æ¥ä¼šè¢«å¼ºåŒ–
        """
        if not self.connection_usage:
            return

        usage_values = list(self.connection_usage.values())
        avg_usage = sum(usage_values) / len(usage_values)

        to_prune = []
        to_strengthen = []

        for conn, usage in self.connection_usage.items():
            if usage < avg_usage * prune_ratio:
                to_prune.append(conn)
            elif usage > avg_usage * strengthen_ratio:
                to_strengthen.append(conn)

        # å‰ªæ‰ä½æ•ˆè¿æ¥
        for conn in to_prune:
            from_unit, to_unit = conn
            if to_unit in self.connections.get(from_unit, {}):
                del self.connections[from_unit][to_unit]  # âœ… åˆ é™¤ dict çš„ key

            print(f"[å‰ªæ] è¿æ¥ {from_unit} â†’ {to_unit} è¢«å‰ªæ‰")
            # ä¹Ÿåˆ æ‰ usageè®°å½•
            if conn in self.connection_usage:
                del self.connection_usage[conn]

        # å¼ºåŒ–é«˜æ•ˆè¿æ¥ï¼ˆå¯é€‰ï¼šæ¯”å¦‚å¢åŠ èƒ½é‡ä¼ é€’æƒé‡ç­‰ï¼‰
        for conn in to_strengthen:
            # ç®€å•æ‰“å°æ ‡è®°ï¼Œå¯ä»¥åç»­åŠ çœŸå®æƒé‡ç³»ç»Ÿ
            print(f"[å¼ºåŒ–] è¿æ¥ {conn[0]} â†’ {conn[1]} è¢«å¼ºåŒ–")

        print(f"[å‰ªæ] å‰ªæ‰ {len(to_prune)} æ¡å¼±è¿æ¥ï¼Œå¼ºåŒ– {len(to_strengthen)} æ¡å¼ºè¿æ¥")

    def auto_connect(self):
        for unit in self.units:
            # åªå¤„ç† processor èŠ‚ç‚¹
            if unit.get_role() != "processor":
                continue

            # è·å–å·²æœ‰è¿æ¥æ•°ï¼ˆä¸‹æ¸¸ï¼‰
            current_connections = self.connections[unit.id]
            if len(current_connections) < 2:
                # éšæœºæ‰¾ä¸€ä¸ª emitter æˆ– processor æ¥è¿æ¥
                def euclidean(p1, p2):
                    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5

                u_pos = unit.get_position()
                candidates = [
                    u for u in self.units
                    if u.id != unit.id and u.get_role() in ["processor", "emitter"] and abs(u.input_size - unit.input_size) <= 100
                       and u.id not in self.connections[unit.id]
                       and euclidean(u_pos, u.get_position()) < 3
                ]
                if not candidates:
                    # âœ… æ²¡æœ‰è¿‘é‚»ä¹Ÿå…è®¸å…¨å±€æœç´¢ emitter å»ºè¿
                    candidates = [u for u in self.units if
                                  u.id != unit.id and u.get_role() in ["processor", "emitter"] and u.id not in
                                  self.connections[unit.id]]

                # æŒ‰èƒ½é‡ä»é«˜åˆ°ä½æ’åºï¼Œä¼˜å…ˆé€‰æ‹©æœ€æœ‰ä»·å€¼è¿æ¥ç›®æ ‡
                if candidates:
                    def connection_strength(u):
                        incoming_count = sum(u.id in self.connections.get(fid, {}) for fid in self.unit_map)
                        return u.energy + incoming_count * 0.1

                    candidates.sort(key=connection_strength, reverse=True)

                    target = candidates[0]  # èƒ½é‡æœ€é«˜è€…
                    if target.id not in current_connections:
                        self.connect(unit, target)
                        print(f"[æ–°è¿æ¥] {unit.id} â†’ {target.id}")
        # === éšæœºçªå˜è¿æ¥ï¼šprocessor æœ‰å°æ¦‚ç‡è¿æ¥æ–°ç›®æ ‡ ===
        if random.random() < 0.1:  # 10% æ¦‚ç‡è§¦å‘çªå˜
            from_candidates = [u for u in self.units if u.get_role() == "processor"]
            to_candidates = [u for u in self.units if u.get_role() in ["processor", "emitter"]]

            if from_candidates and to_candidates:
                from_unit = random.choice(from_candidates)
                to_unit = random.choice(to_candidates)

                if to_unit.id not in self.connections.get(from_unit.id, []):
                    self.connect(from_unit, to_unit)
                    print(f"[çªå˜è¿æ¥] {from_unit.id} â†’ {to_unit.id}")


    # === åˆ†åŒ–æœºåˆ¶ï¼šç»“æ„å¤±è¡¡æ—¶çš„è§’è‰²è°ƒæ•´ ===
    def rebalance_cell_types(self):
        from collections import Counter
        total = len(self.units)
        if total < 15:
            return  # å¤ªå°å…ˆè‡ªç”±ç”Ÿé•¿

        # åŠ¨æ€è¿Ÿæ»çª—å£  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #   æ€»æ•°   <50   <200   <500   500+
        #   hi    1.50  1.30   1.15   1.08
        #   lo    0.50  0.70   0.85   0.92
        if total < 50:
            hi, lo = 1.50, 0.50
        elif total < 200:
            hi, lo = 1.30, 0.70
        elif total < 500:
            hi, lo = 1.15, 0.85
        else:
            hi, lo = 1.10, 0.90

        # Î” å®¹å·®ï¼ˆè‡³å°‘ç›¸å·® Î”_cell æ‰ç®—â€œçœŸçš„å¤šï¼å°‘â€ï¼‰
        delta_cell = max(1, int(total * TOL_FRAC))

        # æœ¬è½®æœ€å¤šè½¬æ¢
        max_conv = max(1, int(total * MAX_CONV_FRAC))
        conv_done = 0

        def pick_weakest(units):
            return min(units, key=lambda u: (u.energy,
                                             getattr(u, "avg_recent_calls", 0.0)))

        while conv_done < max_conv:
            # â”€â”€ é‡æ–°è®¡æ•°
            cnt = Counter(u.get_role() for u in self.units)
            s_cnt = cnt.get("sensor", 0)
            p_cnt = cnt.get("processor", 0)
            e_cnt = cnt.get("emitter", 0)

            desired = {
                "sensor": total * IDEAL_RATIO["sensor"] / DENOM,
                "processor": total * IDEAL_RATIO["processor"] / DENOM,
                "emitter": total * IDEAL_RATIO["emitter"] / DENOM,
            }

            # ratio & diff
            ratio = {
                "sensor": s_cnt / (desired["sensor"] or 1),
                "processor": p_cnt / (desired["processor"] or 1),
                "emitter": e_cnt / (desired["emitter"] or 1),
            }
            diff = {
                "sensor": s_cnt - desired["sensor"],
                "processor": p_cnt - desired["processor"],
                "emitter": e_cnt - desired["emitter"],
            }

            # 1) æ»¡è¶³ ratio>hi ä¸” diffâ‰¥Î” æ‰ç®—â€œoverâ€   2) ratio<lo ä¸” diffâ‰¤-Î” ç®—â€œunderâ€
            overs = [r for r in ratio if ratio[r] > hi and diff[r] >= delta_cell]
            unders = [r for r in ratio if ratio[r] < lo and diff[r] <= -delta_cell]

            if not overs or not unders:
                break  # è½å…¥è¿Ÿæ»å¸¦ or Î” å¤ªå°ï¼Œç»“æŸ

            # é€‰æœ€è¿‡é‡ & æœ€ä¸è¶³
            giver_role = max(overs, key=lambda r: diff[r])  # diff æœ€å¤§
            receiver_role = min(unders, key=lambda r: diff[r])  # diff æœ€å°(è´Ÿæ•°)

            # å– giver_role æœ€å¼±è€…
            cand = [u for u in self.units if u.get_role() == giver_role]
            if not cand:
                break
            unit = pick_weakest(cand)

            # â”€â”€ è½¬åŒ–
            old = unit.get_role()
            unit.role = receiver_role
            unit.age = 0
            unit.energy += 0.2
            unit.gene[f"{receiver_role}_bias"] = 1.0
            print(f"[å¹³è¡¡] {old}â†’{receiver_role} | step={self.current_step}")

            # æ¸…æ—§è¿ & ç®€æ˜“æ–°è¿
            for uid, out_edges in list(self.connections.items()):
                out_edges.pop(unit.id, None)
                self.connection_usage.pop((uid, unit.id), None)
            self.connections[unit.id] = {}

            if receiver_role == "processor":
                tgt = max((u for u in self.units if u.get_role() == "emitter"),
                          default=None, key=lambda u: u.energy)
                if tgt: self.connect(unit, tgt)
            elif receiver_role == "emitter":
                src = max((u for u in self.units if u.get_role() == "processor"),
                          default=None, key=lambda u: u.energy)
                if src: self.connect(src, unit)
            elif receiver_role == "sensor":
                tgt = max((u for u in self.units if u.get_role() == "processor"),
                          default=None, key=lambda u: u.energy)
                if tgt: self.connect(unit, tgt)

            conv_done += 1

    def trace_info_paths(self):
        print(f"[ä¿¡æ¯è·¯å¾„è¿½è¸ª] æ­¥æ•° {self.current_step}")
        for emitter in self.units:
            if emitter.get_role() != "emitter":
                continue

            # è¿½æº¯ä¸Šæ¸¸ processor
            emit_from = [pid for pid in self.unit_map if emitter.id in self.connections.get(pid, {})]
            for pid in emit_from:
                proc_from = [sid for sid in self.unit_map if pid in self.connections.get(sid, {})]
                for sid in proc_from:
                    print(f"  sensor:{sid} â†’ processor:{pid} â†’ emitter:{emitter.id}")

    def _select_clone_parents(self, pending_by_role):
        """
        ä»å¾…å¤åˆ¶çˆ¶å•å…ƒä¸­ï¼ŒæŒ‰ç…§ 10% é…é¢ & èƒ½é‡/æ´»è·ƒåº¦æ’åºæŒ‘å‡ºçœŸæ­£å…è®¸å¤åˆ¶çš„ã€‚
        è¿”å› List[CogUnit]
        """
        total_cells = len(self.units)
        if total_cells <= 15:  # å°è§„æ¨¡é˜¶æ®µä¸è®¾é™
            return [u for lst in pending_by_role.values() for u in lst]

        approved = []
        for role, cand in pending_by_role.items():
            if not cand:
                continue
            role_count = sum(1 for u in self.units if u.role == role)
            cap = max(1, role_count // 15)  # 15 %ï¼Œå‘ä¸‹å–æ•´ï¼Œè‡³å°‘ 1
            cand.sort(key=lambda u: (u.energy, u.avg_recent_calls), reverse=True)
            approved.extend(cand[:cap])
        return approved

    def step(self, input_tensor: torch.Tensor):
        if self.current_step == 10000:
            self.subsystem_competition = True
            print("[è¿›åŒ–] å­ç³»ç»Ÿç«äº‰æœºåˆ¶å·²æ¿€æ´»ï¼ˆSubsystem Competitionï¼‰")

        if self.current_step == 2000:
            for unit in self.units:
                unit.dynamic_aging = True
            print("[è¿›åŒ–] åŠ¨æ€å¯¿å‘½æœºåˆ¶å·²æ¿€æ´»ï¼ˆDynamic Agingï¼‰")

        if self.current_step > 3000 and self.current_step % 100 == 0:
            total_e = self.total_energy()
            if total_e > self.max_total_energy * 2.5:  # è¶…è¿‡åˆå§‹æœ€å¤§èƒ½é‡2.5å€
                tax = total_e * 0.01
                loss_per_unit = tax / max(len(self.units), 1)
                for unit in self.units:
                    unit.energy -= loss_per_unit  # âœ… å®é™…æ‰£èƒ½é‡
                print(f"[èƒ½é‡ç¨] ç¬¬ {self.current_step} æ­¥ï¼Œæ€»èƒ½é‡è¿‡é«˜ï¼Œæ‰£é™¤ {tax:.2f} èƒ½é‡")

        # === Curriculum Learning: æ¯500æ­¥æ‰©å±•ä¸€æ¬¡ç¯å¢ƒå¤§å°
        if self.current_step > 0 and self.current_step % 500 == 0:
            old_size = self.env_size
            self.env_size = min(self.env_size + 5, 20)  # æ¯æ¬¡+5ï¼Œæœ€å¤§åˆ°20x20
            self.env = GridEnvironment(size=self.env_size)  # é‡æ–°ç”Ÿæˆç¯å¢ƒ
            self.upscale_old_units(self.env_size * self.env_size * INPUT_CHANNELS)

            new_target = (random.randint(0, self.env_size - 1), random.randint(0, self.env_size - 1))
            self.task = TaskInjector(target_position=new_target)
            self.target_vector = self.task.encode_goal(self.env_size)
            print(
                f"[Curriculumå‡çº§] ç¬¬ {self.current_step} æ­¥ï¼šç¯å¢ƒå¤§å° {old_size}x{old_size} â†’ {self.env_size}x{self.env_size}ï¼Œæ–°ç›®æ ‡ {new_target}")

        if self.current_step > 0 and self.current_step % 100 == 0:
            old_max = self.max_total_energy
            self.max_total_energy *= 2
            print(f"[èµ„æºæ‰©å±•] ç¬¬ {self.current_step} æ­¥ï¼šMAX_TOTAL_ENERGY {old_max:.1f} â†’ {self.max_total_energy:.1f}")

        # è‹¥å½“å‰æ­¥æ•°éå¸¸æ—©æœŸï¼Œç»™äºˆåŸºç¡€èƒ½é‡è¡¥å¿
        if self.current_step < 10:
            for unit in self.units:
                if unit.get_role() != "sensor":
                    unit.energy += 0.1
                    print(f"[é¢„çƒ­è¡¥å¿] {unit.id} åˆå§‹é˜¶æ®µè·å¾—èƒ½é‡ +0.1")

        if self.current_step > 0 and self.current_step % 100 == 0:
            old_target = self.target_vector.clone()
            self.target_vector = torch.rand_like(self.target_vector)

            similarity = torch.cosine_similarity(old_target, self.target_vector, dim=0).item()
            print(f"[ç›®æ ‡å˜åŒ–] ç¬¬ {self.current_step} æ­¥ï¼Œtarget_vector æ›´æ–°ï¼ï¼ˆç›¸ä¼¼åº¦ {similarity:.3f}ï¼‰")

        if self.current_step > 0 and self.current_step % 100 == 0:
            self.prune_connections()

        if self.current_step > 0 and self.current_step % 300 == 0:
            self.assign_subsystems()

        if hasattr(self, "subsystem_competition") and self.subsystem_competition:
            if self.current_step % 1000 == 0:
                subsystem_energies = {}
                for unit in self.units:
                    if unit.subsystem_id:
                        subsystem_energies.setdefault(unit.subsystem_id, 0)
                        subsystem_energies[unit.subsystem_id] += unit.energy

                if len(subsystem_energies) >= 5:  # è‡³å°‘5ä¸ªå­ç³»ç»Ÿæ‰ç«äº‰
                    weakest = min(subsystem_energies, key=lambda x: subsystem_energies[x])
                    print(f"[å­ç³»ç»Ÿç«äº‰] æ·˜æ±°èƒ½é‡æœ€å¼±çš„å­ç³»ç»Ÿ {weakest}")

                    # åˆ é™¤å¼±å­ç³»ç»Ÿçš„æ‰€æœ‰å•å…ƒ
                    self.units = [u for u in self.units if u.subsystem_id != weakest]
                    self.unit_map = {u.id: u for u in self.units}
                    self.connections = {u.id: {} for u in self.units}

        self.current_step += 1


        # è®¡ç®—å½“å‰å„è§’è‰²å•å…ƒæ€»æ•°ï¼Œä¾›ç´§æ€¥å¢æ®–åˆ¤æ–­ä½¿ç”¨
        sensor_count = sum(1 for u in self.units if u.get_role() == "sensor")
        processor_count = sum(1 for u in self.units if u.get_role() == "processor")
        emitter_count = sum(1 for u in self.units if u.get_role() == "emitter")
        for unit in self.units:
            unit.global_sensor_count = sensor_count
            unit.global_processor_count = processor_count
            unit.global_emitter_count = emitter_count
        for unit in self.units:
            unit.global_unit_count = len(self.units)

        new_units = []  # æ–°ç”Ÿæˆçš„å•å…ƒï¼ˆå¤åˆ¶ï¼‰
        pending = {"sensor": [], "processor": [], "emitter": []}  # NEW: å¾…å¤åˆ¶çˆ¶å•å…ƒ
        output_buffer = {}  # ç¼“å­˜æ¯ä¸ªå•å…ƒçš„è¾“å‡º {unit_id: output_tensor}

        # === ç³»ç»Ÿæ€»èƒ½é‡é™åˆ¶ï¼Œä¿æŠ¤ clone ===
        allow_clone = self.total_energy() < self.max_total_energy

        # === ç¬¬ä¸€é˜¶æ®µï¼šå•å…ƒæ›´æ–°å¤„ç† ===
        # ç»Ÿè®¡å½“å‰ emitter æ•°é‡
        emitter_count = sum(1 for u in self.units if u.get_role() == "emitter")

        for unit in self.units:
            unit.global_emitter_count = emitter_count


        for unit in self.units[:]:
            env_state = torch.from_numpy(self.env.get_state()).float()
            goal_tensor = self.task.encode_goal(self.env_size)
            unit_input = torch.cat([env_state, goal_tensor], dim=0).unsqueeze(0)

            # å¦‚æœè¯¥å•å…ƒæœ‰ä¸Šæ¸¸è¿æ¥ï¼ˆè¢«å…¶ä»–å•å…ƒæŒ‡å‘ï¼‰
            incoming = [uid for uid in self.unit_map if unit.id in self.connections.get(uid, {})]
            for uid in list(self.unit_map):
                if unit.id in self.connections.get(uid, {}):  # dict not list
                    self.connection_usage[(uid, unit.id)] = self.current_step

                    self.connections[uid][unit.id] = min(self.connections[uid][unit.id], 5.0)  # ä¸Šé™

            if unit.get_role() == "sensor":
                env_state = torch.from_numpy(self.env.get_state()).float()
                goal_tensor = self.task.encode_goal(self.env_size)
                sensor_input = torch.cat([env_state, goal_tensor], dim=0)
                unit_input = sensor_input.unsqueeze(0)

            elif incoming:
                weighted_outputs = []
                total_weight = 0.0
                for uid in incoming:
                    strength = self.connections[uid][unit.id]
                    output = self.unit_map[uid].get_output().squeeze(0)  # ç»Ÿä¸€ä¸º [8]
                    target_len = self.env_size * self.env_size * INPUT_CHANNELS
                    if output.shape[0] != target_len:
                        padding = (0, target_len - output.shape[0])

                        output = torch.nn.functional.pad(output, padding, value=0)

                    weighted_outputs.append(output * strength)
                    total_weight += strength

                if total_weight > 0:
                    unit_input = torch.stack(weighted_outputs).sum(dim=0, keepdim=True) / total_weight
                else:
                    unit_input = torch.zeros_like(input_tensor).unsqueeze(0)
            else:
            # å¼ºåˆ¶ä½¿ç”¨é›¶è¾“å…¥è§¦å‘æ›´æ–°ï¼Œé¿å…å› æ— è¾“å…¥æ°¸è¿œä¸æ›´æ–°
                unit_input = torch.zeros(unit.input_size).unsqueeze(0)
                print(f"[é›¶è¾“å…¥] {unit.id} æ— ä¸Šæ¸¸è¿æ¥ï¼Œä½¿ç”¨é›¶è¾“å…¥æ›´æ–°")

            # æ‰§è¡Œå•å…ƒçš„æ›´æ–°é€»è¾‘
            # === ç»Ÿè®¡è°ƒç”¨é¢‘ç‡ï¼ˆè¿™é‡Œå¯ä»¥æ›´ç²¾ç»†ï¼Œæ¯”å¦‚ sliding windowï¼‰===
            incoming = [uid for uid in self.unit_map if unit.id in self.connections.get(uid, {})]
            unit.recent_calls = len(incoming)  # è¡¨ç¤ºè¿™ä¸ªå•å…ƒåœ¨è¿™ä¸€æ­¥è¢«å¤šå°‘ä¸Šæ¸¸è°ƒç”¨
            unit.connection_count = len(self.connections.get(unit.id, {}))  # ç›´æ¥ä¸‹æ¸¸è¿æ¥æ•°é‡
            # æ›´æ–°è°ƒç”¨å†å²è®°å½•
            unit.call_history.append(unit.recent_calls)
            if len(unit.call_history) > unit.call_window:
                unit.call_history.pop(0)

            # è®¡ç®—å¹³å‡è°ƒç”¨é¢‘ç‡
            unit.avg_recent_calls = sum(unit.call_history) / len(unit.call_history)

            if unit.recent_calls == 0:
                unit.inactive_steps += 1
            else:
                unit.inactive_steps = 0  # é‡ç½®

            unit.current_step = self.current_step
            # === ç»Ÿä¸€åŠ¨æ€èƒ½é‡æ¶ˆè€— ===
            var = torch.var(unit_input).item()
            freq = unit.recent_calls
            conn = unit.connection_count
            call_density = freq / (conn + 1)
            conn_strength_sum = sum(self.connections.get(unit.id, {}).values())

            # ç»Ÿä¸€ä»£è°¢æ¨¡å‹ï¼ˆå¯è°ƒæƒé‡ï¼‰
            dim_scale = 50.0 / unit.input_size  # â† 50 æ˜¯æœ€åˆç¯å¢ƒ(5Ã—5Ã—2)çš„åŸºå‡†
            bias_factor = 1.0
            if unit.role == "sensor":
                bias_factor = unit.gene.get("sensor_bias", 1.0)
            elif unit.role == "processor":
                bias_factor = unit.gene.get("processor_bias", 1.0)
            elif unit.role == "emitter":
                bias_factor = unit.gene.get("emitter_bias", 1.0)

            decay = (var * 0.1 + call_density * 0.005 + conn_strength_sum * 0.003) * dim_scale * bias_factor

            unit.energy -= decay
            unit.energy = max(unit.energy, 0.0)

            print(
                f"[ä»£è°¢] {unit.id} var={var:.3f}, freq={freq}, conn={conn}, strength_sum={conn_strength_sum:.2f} â†’ -{decay:.3f} èƒ½é‡")

            unit.update(unit_input)
            # âœ… åŠ å¼ºè¿æ¥æƒé‡ï¼ˆä½¿ç”¨æ¬¡æ•°è¶Šå¤šè¶Šå¼ºï¼‰
            for uid in incoming:
                if unit.id in self.connections.get(uid, {}):
                    self.connections[uid][unit.id] *= 1.05  # å¢å¼º
                    self.connections[uid][unit.id] = min(self.connections[uid][unit.id], 5.0)
            output_buffer[unit.id] = unit.get_output()
            print(unit)

            # === åˆ¤æ–­æ˜¯å¦éœ€è¦å¤åˆ¶ ===
            if allow_clone and unit.should_split():
                pending[unit.role].append(unit)   # åªè®°å½•çˆ¶å•å…ƒï¼Œä¸ç«‹å³ clone


            else:
                if not allow_clone:
                    print(f"[ç³»ç»Ÿä¿æŠ¤] æ€»èƒ½é‡è¿‡é«˜ï¼Œç¦æ­¢ {unit.id} åˆ†è£‚")

            # === åˆ¤æ–­æ˜¯å¦æ­»äº¡ ===
            if unit.should_die():
                print(f"[æ­»äº¡] {unit.id} è¢«ç§»é™¤")
                self.remove_unit(unit)


        self.auto_connect()
        # === æ­»è¿æ¥æ¸…ç† ===
        if self.current_step % 10 == 0:
            threshold = 50
            for from_id in list(self.connections.keys()):
                for to_id in list(self.connections[from_id].keys()):
                    last_used = self.connection_usage.get((from_id, to_id), -1)
                    if self.current_step - last_used > threshold:
                        del self.connections[from_id][to_id]  # âœ… æ­£ç¡®åˆ é™¤æ–¹å¼
                        print(f"[æ­»è¿æ¥æ¸…é™¤] {from_id} â†’ {to_id}")
                        # åˆ é™¤è¿æ¥åï¼Œç»™ from_unit è½»å¾®èƒ½é‡æƒ©ç½š
                        if from_id in self.unit_map:
                            self.unit_map[from_id].energy -= 0.01  # å¯è°ƒå‚æ•°
                            print(f"[æƒ©ç½š] {from_id} å› è¿æ¥å¤±æ•ˆï¼Œèƒ½é‡ -0.01")

                    else:
                        # âœ… å‰Šå¼±ä»åœ¨ç”¨ä½†è¡¨ç°å·®çš„è¿æ¥
                        self.connections[from_id][to_id] *= 0.95
                        if self.connections[from_id][to_id] < 0.1:
                            del self.connections[from_id][to_id]
                            print(f"[è¿æ¥è¡°å‡æ¸…é™¤] {from_id} â†’ {to_id}")

        # ç®€æ˜“ä»»åŠ¡å¥–åŠ±ï¼šå¦‚æœ emitter è¾“å‡ºé è¿‘æŸä¸ªç›®æ ‡å‘é‡ï¼Œåˆ™å‘æ”¾å¥–åŠ±
        target_vector = self.target_vector
        outputs = self.collect_emitter_outputs()
        if outputs is not None:
            avg_output = outputs.mean(dim=0)
            distance = torch.norm(avg_output - target_vector)

            # çº¿æ€§è¡°å‡å¼å¥–åŠ±åˆ†æ•°ï¼ˆè·ç¦» 0â†’å¥–åŠ±æ»¡åˆ†1ï¼Œè·ç¦»3â†’å¥–åŠ±ä¸º0ï¼‰
            reward_score = max(0.0, 1.0 - distance / 10.0)

            if reward_score > 0.0:
                dim_scale = self.target_vector.size(0) / 50  # 50 â†’ åŸå§‹åŸºå‡†
                dilution_factor = 1.0
                if self.current_step >= 5000:  # 5000æ­¥åå¼€å§‹å¥–åŠ±ç¨€é‡Š
                    dilution_factor = max(0.5, 1.0 - 0.00005 * (self.current_step - 5000))

                for unit in self.units:
                    if unit.get_role() == "processor":
                        unit.energy += 0.04 * dim_scale * reward_score * dilution_factor
                    elif unit.get_role() == "emitter":
                        unit.energy += 0.02 * dim_scale * reward_score * dilution_factor

                print(f"[å¥–åŠ±] è¾“å‡ºæ¥è¿‘ç›®æ ‡ï¼Œè·ç¦» {distance:.2f}ï¼Œå¥–åŠ±æ¯”ç‡ {reward_score:.2f} â†’ èƒ½é‡åˆ†é…å®Œæ¯•")

            # âœ… å¢åŠ å¤šæ ·æ€§æƒ©ç½šï¼ˆå«æ•°é‡åˆ¤æ–­ï¼‰
            action_indices = [torch.argmax(out).item() for out in outputs]

            if len(action_indices) >= 3:  # è‡³å°‘ 3 ä¸ª emitter æ‰æœ‰ç»Ÿè®¡æ„ä¹‰
                common_action = max(set(action_indices), key=action_indices.count)
                if action_indices.count(common_action) > len(action_indices) * 0.9:
                    for unit in self.units:
                        if unit.get_role() == "emitter":
                            unit.energy -= 0.02
                            print(f"[æƒ©ç½š] emitter {unit.id} å› è¾“å‡ºå•ä¸€è¡Œä¸ºè¢«æ‰£èƒ½é‡")
                elif len(set(action_indices)) > len(action_indices) * 0.6:
                    for unit in self.units:
                        if unit.get_role() == "emitter":
                            unit.energy += 0.02

                    print(f"[å¥–åŠ±] emitter è¾“å‡ºå¤šæ ·æ€§é«˜ â†’ æ‰€æœ‰ emitter +0.02 èƒ½é‡")

            else:
                print(f"[è·³è¿‡å¤šæ ·æ€§æƒ©ç½š] emitter æ•°é‡ä¸è¶³ï¼Œä»… {len(action_indices)} ä¸ª")

            if self.task.evaluate(self.env, outputs):
                print(f"[ä»»åŠ¡å®Œæˆ] è¾¾æˆç›®æ ‡ä½ç½® {self.task.target_position}ï¼Œå¥–åŠ± +0.1")
                for unit in self.units:
                    if unit.get_role() == "processor":
                        unit.energy += 0.1

        self.trace_info_paths()
        # âœ… æ‰§è¡Œç»“æ„å†—ä½™åˆå¹¶
        self.merge_redundant_units()
        self.restructure_common_subgraphs()

        # âœ… æ‰“å°å½“å‰å„ç±»å‹ç»†èƒæ•°é‡
        from collections import Counter
        role_counts = Counter([unit.get_role() for unit in self.units])
        print("[ç»†èƒç»Ÿè®¡] å½“å‰å„ç±»æ•°é‡ï¼š", dict(role_counts))

        print("[è¿æ¥å¼ºåº¦]")
        for from_id, to_dict in self.connections.items():
            for to_id, strength in to_dict.items():
                print(f"  {from_id} â†’ {to_id} = {strength:.3f}")
                # ç»Ÿè®¡å„ç±»å•å…ƒæ•°é‡
                sensor_count = sum(1 for u in self.units if u.get_role() == "sensor")
                processor_count = sum(1 for u in self.units if u.get_role() == "processor")
                emitter_count = sum(1 for u in self.units if u.get_role() == "emitter")
                print(f"[ç»Ÿè®¡] sensor: {sensor_count}, processor: {processor_count}, emitter: {emitter_count}")


        self.rebalance_cell_types()
        # === 10 %-é™é¢å¤åˆ¶ï¼ˆ>15 ç»†èƒæ‰è§¦å‘ï¼‰ ===
        selected_parents = self._select_clone_parents(pending)
        for parent in selected_parents:
            expected_input = self.env_size * self.env_size * INPUT_CHANNELS

            child = parent.clone(
                new_input_size=expected_input if parent.input_size != expected_input else None
            )
            # çˆ¶å­è¿æ¥ï¼ˆå«ç»§æ‰¿ä¸Šä¸‹æ¸¸ï¼‰
            self.connect(parent, child)
            # ç»§æ‰¿ä¸Šæ¸¸
            for uid in list(self.unit_map):
                if parent.id in self.connections.get(uid, {}):
                    self.connect(self.unit_map[uid], child)
            # ç»§æ‰¿ä¸‹æ¸¸
            for uid in self.connections.get(parent.id, {}):
                if uid in self.unit_map:
                    self.connect(child, self.unit_map[uid])
            new_units.append(child)
            # â€”â€” æœ€ç»ˆä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ child åŠ å…¥å›¾ç»“æ„ â€”â€”
        for unit in new_units:
            unit.memory_pool = self.memory_pool  # è‡ªåŠ¨æ³¨å…¥é—ä¼ è®°å¿†æ± 
            self.add_unit(unit)



    def upscale_old_units(self, new_input_size):
        """å°†æ‰€æœ‰ input_size å°äºå½“å‰ç¯å¢ƒé¢„æœŸå°ºå¯¸çš„å•å…ƒå‡ç»´ï¼ˆåªå‡ä¸é™ï¼‰"""
        for unit in self.units:
            if unit.input_size < new_input_size:
                print(f"[å‡ç»´] {unit.id} input_size {unit.input_size} â†’ {new_input_size}")

                # ä¿ç•™æ—§è¾“å‡ºéƒ¨åˆ†ï¼Œè¡¥é›¶åˆ°æ–°ç»´åº¦
                old_output = unit.last_output
                if old_output.dim() == 2 and old_output.shape[0] == 1:
                    old_output = old_output.squeeze(0)

                padded_output = torch.zeros(new_input_size)
                padded_output[:old_output.shape[0]] = old_output
                unit.last_output = padded_output

                # åŒç†ï¼Œstate ä¹Ÿå‡ç»´
                if unit.state.shape[0] < new_input_size:
                    padded_state = torch.zeros(new_input_size)
                    old_state = unit.state.squeeze(0) if unit.state.dim() == 2 else unit.state
                    padded_state[:old_state.shape[0]] = old_state
                    unit.state = padded_state

                # é‡å»ºç½‘ç»œç»“æ„ï¼ˆéšè—å±‚ç»´åº¦ä¸å˜ï¼‰
                unit.function = torch.nn.Sequential(
                    torch.nn.Linear(new_input_size, unit.hidden_size),
                    torch.nn.ReLU(),
                    torch.nn.Linear(unit.hidden_size, new_input_size)
                )

                unit.input_size = new_input_size

    def summary(self):
        # æ‰“å°å½“å‰å›¾ç»“æ„æ¦‚å†µ

        print(f"[å›¾ç»“æ„] å½“å‰å•å…ƒæ•°: {len(self.units)}")
        for unit in self.units:
            print(f" - {unit} â†’ è¿æ¥æ•°: {len(self.connections[unit.id])}")

    def collect_emitter_outputs(self):
        """æ”¶é›†æ‰€æœ‰ emitter è¾“å‡ºå¹¶è‡ªåŠ¨å¯¹é½åˆ°ç›®æ ‡ç»´åº¦"""
        aligned = []
        for unit in self.units:
            if unit.get_role() != "emitter":
                continue

            raw = unit.get_output().squeeze(0) if unit.get_output().dim() == 2 else unit.get_output()
            vec = self._align_to_goal_dim(raw)

            if vec.shape[-1] != self._goal_dim():
                # ç†è®ºä¸ä¼šå‘ç”Ÿï¼Œå®‰å…¨æ£€æŸ¥
                print(f"[è­¦å‘Š] å¯¹é½å¤±è´¥ {unit.id} é•¿åº¦ {vec.shape[-1]}")
                continue

            aligned.append(vec.unsqueeze(0))

        if aligned:
            stacked = torch.cat(aligned, dim=0)      # [N, goal_dim]
            print("[è¾“å‡ºæ£€æŸ¥] Emitter å¯¹é½åå‡å€¼(å‰5) :", stacked.mean(dim=0)[:5])
            return stacked
        else:
            print("[è¾“å‡ºæ£€æŸ¥] å½“å‰æ²¡æœ‰æ´»è·ƒçš„ emitter å•å…ƒ")
            return None



def interpret_emitter_output(output_tensor):
    """
    å°† emitter çš„è¾“å‡ºå‘é‡è§£é‡Šä¸ºåŠ¨ä½œã€‚
    """
    action_names = [f"åŠ¨ä½œ{i}" for i in range(50)]
    if output_tensor.dim() == 3:
        output_tensor = output_tensor.squeeze(1)  # å˜æˆ [N, 8]

    for i, out in enumerate(output_tensor):
        raw_index = torch.argmax(out).item()
        action_index = raw_index % 4  # ğŸŒŸ æŠ˜å åˆ° 0~3
        action = ["ä¸Š", "ä¸‹", "å·¦", "å³"][action_index]  # æˆ–è€…è‡ªå®šä¹‰åŠ¨ä½œåç§°
        print(f"[è¡Œä¸ºè§¦å‘] ç¬¬ {i + 1} ä¸ª emitter æ‰§è¡ŒåŠ¨ä½œ: {action}ï¼ˆåŸå§‹ index = {raw_index}ï¼‰")


def environment_feedback(output_tensor, graph):
    """
    ç¯å¢ƒå¯¹ emitter è¾“å‡ºçš„ç®€å•åé¦ˆæœºåˆ¶ï¼š
    - å¦‚æœè¾“å‡ºä¸­å‡ºç°ç‰¹å®šæ¨¡å¼ï¼ˆä¾‹å¦‚ â†‘ å’Œ â†’ åŒæ—¶è¾ƒå¼ºï¼‰ï¼Œå¥–åŠ±å¯¹åº” emitter
    - å¥–åŠ±é€šè¿‡æå‡ energy å®ç°
    """
    if output_tensor.dim() == 3:
        output_tensor = output_tensor.squeeze(1)  # [N, 8]

    for i, out in enumerate(output_tensor):
        # ç¤ºä¾‹æ¡ä»¶ï¼šè‹¥ â†‘ (index 0) å’Œ â†’ (index 3) è¾“å‡ºå€¼éƒ½å¤§äº 0.2
        if out[5] > 0.3 and out[13] < -0.1:  # è‡ªå®šä¹‰è§„åˆ™
            emitter = [u for u in graph.units if u.get_role() == "emitter"][i]
            emitter.energy += 0.05  # ç®€å•å¥–åŠ±
            print(f"[å¥–åŠ±] emitter {emitter.id} å›  â†‘+â†’ è¢«å¥–åŠ± +0.05 èƒ½é‡")



if __name__ == "__main__":
    env = GridEnvironment(size=5)
    # åˆå§‹åŒ–ä»»åŠ¡ç›®æ ‡ï¼ˆä¾‹å¦‚ç›®æ ‡ä½ç½®åœ¨å³ä¸‹è§’ (4, 4)ï¼‰
    task = TaskInjector(target_position=(4, 4))
    goal_tensor = task.encode_goal(env.size)  # ç”Ÿæˆ 25ç»´ one-hot å‘é‡
    graph = CogGraph()

    # åˆå§‹åŒ–å•å…ƒ
    sensor = CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS
, role="sensor")
    emitter = CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS
, role="emitter")


    # å¤šä¸ª processor
    processor_list = []
    for _ in range(4):
        p = CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS
, role="processor")
        processor_list.append(p)



    # åŠ å…¥å›¾ç»“æ„
    graph.add_unit(sensor)
    graph.add_unit(emitter)
    for p in processor_list:
        graph.add_unit(p)

    # å»ºç«‹è¿æ¥
    for p in processor_list:
        graph.connect(sensor, p)
        graph.connect(p, emitter)

    # --- æ–°å¢ï¼šé¢å¤–ç§å­ ---
    extra_sensors = [CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS,
                             role="sensor") for _ in range(1)]  # å†è¡¥ 1 ä¸ª
    extra_emitters = [CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS,
                              role="emitter") for _ in range(1)]  # å†è¡¥ 1 ä¸ª
    for u in extra_sensors + extra_emitters:
        graph.add_unit(u)
        # è®©æ¯ä¸ª sensor â†’ processor[0]ï¼Œprocessor[-1] â†’ æ¯ä¸ª emitterï¼Œä¿è¯ä¿¡æ¯é€šè·¯
        graph.connect(u, processor_list[0]) if u.role == "sensor" else graph.connect(processor_list[-1], u)

    # è¿è¡Œæ¨¡æ‹Ÿ
    for step in range(500):
        print(f"\n==== ç¬¬ {step+1} æ­¥ ====")
        # 1ï¸âƒ£ è·å–ç¯å¢ƒçŠ¶æ€ï¼Œè¾“å…¥ sensor
        state = env.get_state()
        input_tensor = torch.cat([
            torch.from_numpy(state).float(),
            goal_tensor
        ], dim=0)

        # 2ï¸âƒ£ å¯åŠ¨è®¤çŸ¥ç³»ç»Ÿï¼ˆæ„ŸçŸ¥ + å†³ç­–ï¼‰
        graph.step(input_tensor)

        # 3ï¸âƒ£ æ”¶é›† emitter è¾“å‡ºï¼Œè½¬æ¢ä¸ºåŠ¨ä½œ
        emitter_output = graph.collect_emitter_outputs()
        if emitter_output is not None:
            raw_idx = torch.argmax(emitter_output.mean(dim=0)).item()
            action_index = raw_idx % 4  # æŠ˜å åˆ° 0-3
            print(f"[è¡ŒåŠ¨å†³ç­–] æ‰§è¡ŒåŠ¨ä½œ: {action_index}")

            # 4ï¸âƒ£ æ‰§è¡ŒåŠ¨ä½œï¼Œæ”¹å˜ç¯å¢ƒ
            env.step(action_index)
            # ğŸ” å°†ç¯å¢ƒèƒ½é‡å˜åŒ–åé¦ˆç»™ emitter
            for unit in graph.units:
                if unit.get_role() == "emitter":
                    unit.energy += env.agent_energy_gain
                    unit.energy -= env.agent_energy_penalty
                    print(f"[ç¯å¢ƒåé¦ˆ] {unit.id} +{env.agent_energy_gain:.2f} -{env.agent_energy_penalty:.2f}")

        # 5ï¸âƒ£ æ‰“å°ç¯å¢ƒå›¾ç¤º
        env.render()

        # å¦‚æœæ²¡æœ‰å•å…ƒå‰©ä¸‹ï¼Œé€€å‡ºd
        if not graph.units:
            print("[ç»ˆæ­¢] æ‰€æœ‰å•å…ƒæ­»äº¡ã€‚")
            break

from collections import Counter
final_counts = Counter([unit.get_role() for unit in graph.units])
print("\nğŸ§¬ æœ€ç»ˆç»†èƒæ€»æ•°ç»Ÿè®¡ï¼š", dict(final_counts))
print("ğŸ”¢ æ€»ç»†èƒæ•° =", len(graph.units))

