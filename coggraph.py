# coggraph.py
import uuid
from CogUnit import CogUnit
import torch
import random
from env import GridEnvironment
import torch.nn.functional as F
import numpy as np
import logging
from collections import deque, Counter
from env import logger



# class LimitedDebugHandler(logging.Handler):
#     def __init__(self, capacity=100):
#         super().__init__(level=logging.DEBUG)  # åªå¤„ç† DEBUG
#         self.buffer = deque(maxlen=capacity)
#
#     def emit(self, record):
#         if record.levelno == logging.DEBUG:
#             try:
#                 msg = self.format(record)
#                 self.buffer.append(msg)
#             except Exception:
#                 pass  # é˜²æ­¢æ ¼å¼åŒ–æŠ¥é”™
#
#     def dump_to_console(self):
#         print("\n==== [æœ€è¿‘ Debug æ—¥å¿—] ====")
#         for msg in self.buffer:
#             print(msg)
#
# # === è®¾ç½® root logger ===
# logger = logging.getLogger()
# logger.setLevel(logging.INFO)
# logger.handlers.clear()  # âœ… é˜²æ­¢é‡å¤æ‰“å°ï¼ˆå…³é”®ä¸€æ­¥ï¼ï¼‰
#
# # âœ… æ·»åŠ  Debug ç¼“å­˜ Handlerï¼ˆä¸ä¼šæ˜¾ç¤ºã€ä¸è¾“å‡ºã€ä»…å†…å­˜ï¼‰
# debug_handler = LimitedDebugHandler(capacity=100)
# debug_handler.setFormatter(logging.Formatter('%(asctime)s [DEBUG] %(message)s', datefmt='%H:%M:%S'))
# logger.addHandler(debug_handler)
#
# # âœ… æ·»åŠ æ­£å¸¸è¾“å‡º Handlerï¼ˆåªæ˜¾ç¤º INFO åŠä»¥ä¸Šï¼‰
# console_handler = logging.StreamHandler()
# console_handler.setLevel(logging.DEBUG)
# console_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
# logger.addHandler(console_handler)




MAX_CONNECTIONS = 4  # æ¯ä¸ªå•å…ƒæœ€å¤šè¿æ¥ 4 ä¸ªä¸‹æ¸¸
N_STATE_CHANNELS = 3
N_GOAL_CHANNELS = 1
INPUT_CHANNELS = N_STATE_CHANNELS + N_GOAL_CHANNELS





class TaskInjector:
    def __init__(self, target_position):
        self.target_position = target_position  # ç›®æ ‡åæ ‡ (x, y)

    def encode_goal(self, env_size):
        """å°†ç›®æ ‡ä½ç½®ç¼–ç æˆ one-hot å‘é‡ï¼ˆä¸è¾“å…¥åŒç»´åº¦ï¼‰"""
        index = self.target_position[1] * env_size + self.target_position[0]
        vec = torch.zeros(env_size * env_size)
        vec[index] = 1.0
        return vec

    def evaluate(self, env, emitter_outputs):
        """è¯„ä¼° emitter æ˜¯å¦â€œæŒ‡å‘â€ç›®æ ‡ä½ç½®"""
        if emitter_outputs is None:
            return False

        pred_index = torch.argmax(emitter_outputs.mean(dim=0)).item()
        x, y = pred_index % env.size, pred_index // env.size
        return (x, y) == self.target_position



# ç†æƒ³æ¯”ä¾‹  emitter : processor : sensor = 1 : 2 : 1
IDEAL_RATIO = {"emitter": 1, "processor": 2, "sensor": 1}
DENOM = sum(IDEAL_RATIO.values())      # =4

# æ¯è½®å…è®¸è½¬æ¢çš„æœ€é«˜æ¯”ä¾‹ï¼ˆ15%ï¼‰
MAX_CONV_FRAC = 0.4

# Î” å®¹å·®ç³»æ•°ï¼šéœ€è¦è‡³å°‘ diff â‰¥ ceil(TOL_FRAC*total) æ‰è§¦å‘
TOL_FRAC = 0.05      # å°è§„æ¨¡æ—¶è‡ªåŠ¨é€€åŒ–æˆ 1


class CogGraph:
    """
    CogGraph ç®¡ç†æ‰€æœ‰ CogUnit çš„é›†åˆå’Œè¿æ¥å…³ç³»ï¼š
    - æ·»åŠ  / åˆ é™¤å•å…ƒ
    - ç®¡ç†è¿æ¥ï¼ˆå¯æ‹“å±•ä¸ºå›¾ï¼‰
    - è°ƒåº¦æ¯ä¸€è½®æ‰€æœ‰ CogUnit çš„æ›´æ–°ã€åˆ†è£‚ã€æ­»äº¡ï¼Œå¹¶ä¼ é€’è¾“å‡º
    """

    # -------------------------------------------------------------------
    # è‡ªåŠ¨ç”Ÿæˆç§å­ç»†èƒï¼ˆsensor=1, processor=4, emitter=1ï¼Œå¯è°ƒï¼‰
    def _init_seed_units(self,
                         n_sensor: int = 2,
                         n_processor: int = 4,
                         n_emitter: int = 2,
                         device: str = "cpu"):

        expected_input = self.env_size * self.env_size * INPUT_CHANNELS

        # 1) åˆ›å»º
        sensors = [CogUnit(input_size=expected_input, role="sensor") for _ in range(n_sensor)]
        processors = [CogUnit(input_size=expected_input, role="processor") for _ in range(n_processor)]
        emitters = [CogUnit(input_size=expected_input, role="emitter") for _ in range(n_emitter)]

        # 2) è¿ç§»åˆ°ç›®æ ‡ device
        for u in sensors + processors + emitters:
            u.to(device)

        # 3) åŠ å…¥å›¾
        for u in sensors + processors + emitters:
            self.add_unit(u)

        # 4) è¿æ¥ï¼šsensor â†’ processor â†’ emitter
        for s in sensors:
            for p in processors:
                self.connect(s, p)
        for p in processors:
            for e in emitters:
                self.connect(p, e)

    # -------------------------------------------------------------------
    def __init__(self, device: str = "cpu"):
        self.device = torch.device(device)
        # === RL æ¥å£ï¼šProcessor è¾“å‡ºçš„ç»Ÿä¸€ç»´åº¦ ===
        self.debug = False
        self.reverse_connections = {}  # to_id -> set(from_ids)
        self.sensor_count = 0
        self.processor_count = 0
        self.emitter_count = 0
        self.energy_pool = 0.0  # ä¸­å¤®èƒ½é‡æ± 
        # self.memory_pool = []  # å­˜æ”¾æ­»äº¡ç»†èƒçš„ gene + last_output + bias info
        self.env_size = 5  # åˆå§‹ç¯å¢ƒ 5x5
        self.env = GridEnvironment(size=self.env_size)  # åˆ›å»ºç¯å¢ƒ
        self.task = TaskInjector(target_position=(self.env_size - 1, self.env_size - 1))  # åˆå§‹ç›®æ ‡ç‚¹
        self.target_vector = self.task.encode_goal(self.env_size)  # åˆå§‹ç›®æ ‡å‘é‡
        self.max_total_energy = 250  # åˆå§‹æœ€å¤§æ€»èƒ½é‡
        self.target_vector = self.task.encode_goal(self.env_size)
        self.connection_usage = {}  # {(from_id, to_id): last_used_step}
        self.current_step = 0
        self.units = []
        self.connections = {}  # {from_id: {to_id: strength_float}}
        self.unit_map = {}     # {unit_id: CogUnit å®ä¾‹} å¿«é€Ÿç´¢å¼•å•å…ƒ
        self.processor_hidden_size = self.env_size * self.env_size * INPUT_CHANNELS
        # --- åœ¨ __init__() çš„æœ€åè°ƒç”¨ ---
        self._init_seed_units(device=device)


    def _update_global_counts(self):
        total = len(self.units)
        self.sensor_count    = sum(1 for u in self.units if u.get_role()=="sensor")
        self.processor_count = sum(1 for u in self.units if u.get_role()=="processor")
        self.emitter_count   = sum(1 for u in self.units if u.get_role()=="emitter")
        # åŠ¨æ€è®¡ç®—ç›®æ ‡å®¹é‡ï¼šä¾‹å¦‚  max(50, total//2)  éšç»†èƒæ•°çº¿æ€§å¢é•¿
        target_mem_cap = max(50, total // 2)
        for u in self.units:
            u.global_sensor_count    = self.sensor_count
            u.global_processor_count = self.processor_count
            u.global_emitter_count   = self.emitter_count
            u.global_unit_count      = total

    def _log_stats_and_conns(self):
        """é›†ä¸­æ‰“å°ä¸€æ¬¡ç»Ÿè®¡ & è¿æ¥å¼ºåº¦ï¼Œé¿å…æ•£è½åœ¨å†…å±‚å¾ªç¯é‡Œé‡å¤è®¡ç®—"""
        # åªæœ‰åœ¨ debug æ¨¡å¼ä¸‹æ‰è¾“å‡º
        if not self.debug:
            return
        # æ¯ 50 æ­¥ æˆ–è€…å‰ 10 æ­¥æ‰æ‰“å°
        if self.current_step % 50 != 0 and self.current_step >= 10:
            return

        # å¿«é€Ÿç®—ä¸€æ¬¡
        s = sum(1 for u in self.units if u.get_role()=="sensor")
        p = sum(1 for u in self.units if u.get_role()=="processor")
        e = sum(1 for u in self.units if u.get_role()=="emitter")
        logger.info(f"[ç»Ÿè®¡] step={self.current_step} | sensor:{s}, processor:{p}, emitter:{e}")

        # å†æŠŠæ‰€æœ‰è¿æ¥å¼ºåº¦ dump ä¸€é
        logger.debug("[è¿æ¥å¼ºåº¦]")
        for frm, to_dict in self.connections.items():
            for to, strg in to_dict.items():
                logger.debug(f"  {frm} â†’ {to} = {strg:.3f}")

    def add_unit(self, unit: CogUnit):
        # --- è‹¥å›¾ä¸­å·²æœ‰å•å…ƒï¼Œåˆ™è®©æ–°å•å…ƒè·Ÿéšå®ƒä»¬çš„ device ---
        if self.units:
            target_device = self.units[0].device
            if unit.device != target_device:
                unit.to(target_device)
        # -----------------------------------------------
        # å°†å•å…ƒåŠ å…¥å›¾ç»“æ„ä¸­
        self.units.append(unit)
        self.unit_map[unit.id] = unit
        self.connections[unit.id] = {}
        self._update_global_counts()

    def _get_min_target_counts(self):
        """
        æ ¹æ®å½“å‰ max_total_energy å’Œè§’è‰²æ¯”ä¾‹ï¼Œè¿”å›æ¯ç±»è§’è‰²çš„æœ€å°å»ºè®®æ•°é‡ã€‚
        """
        total_target = int(self.max_total_energy / 2.6 * 0.9)  # ç³»ç»Ÿæœ€å¤§ç»†èƒæ•° Ã— 0.9 å®‰å…¨ç³»æ•°

        # ç†æƒ³æ¯”ä¾‹ï¼š1(sensor) : 2(processor) : 1(emitter) â†’ æ€»å…± 4 ä»½
        IDEAL_RATIO = {"sensor": 1, "processor": 2, "emitter": 1}
        DENOM = sum(IDEAL_RATIO.values())  # = 4

        target_counts = {
            role: int(total_target * IDEAL_RATIO[role] / DENOM)
            for role in IDEAL_RATIO
        }
        return target_counts

    def remove_unit(self, unit: CogUnit):


        if unit.id not in self.unit_map:
            return  # å·²ç»è¢«åˆ é™¤

        # âœ… é—äº§æœºåˆ¶ï¼šå¯¿ç»ˆæ­£å¯æ—¶ï¼Œèƒ½é‡åˆ†é…ç»™å¹´è½»åè¾ˆ
        if getattr(unit, "death_by_aging", False) and unit.energy > 0.0:
            heirs = [u for u in self.units if u.role == unit.role and u.age < 240 and u.id != unit.id]
            if heirs:
                per_gain = unit.energy / len(heirs)
                for u in heirs:
                    u.energy += per_gain
                logger.info(
                    f"[å¯¿ç»ˆèƒ½é‡ç»§æ‰¿] {unit.id} æ­»äº¡ï¼ˆ{unit.role}ï¼‰ â†’ èƒ½é‡ {unit.energy:.2f} åˆ†ç»™ {len(heirs)} ä¸ªåŒç±»å¹´è½»å•å…ƒï¼Œæ¯äºº +{per_gain:.2f}")

        # âœ… åŠ å…¥åˆ°åŒç±»å±€éƒ¨è®°å¿†æ± 
        if unit.is_worthy_of_memory():
            for other in self.units:
                if other.role == unit.role:
                    other.local_memory_pool.append({
                        "gene": unit.gene.copy(),
                        "output": unit.last_output.clone(),
                        "role": unit.role,
                        "hidden_size": unit.hidden_size,
                        "score": 0
                    })
                    # æ§åˆ¶å¤§å°ï¼šæ¯ä¸ªå•å…ƒæ± æœ€å¤š150æ¡
                    if len(other.local_memory_pool) >= 150:
                        other.local_memory_pool.pop(0)

        # ä»å›¾ä¸­ç§»é™¤å•å…ƒåŠå…¶è¿æ¥
        self.units = [u for u in self.units if u.id != unit.id]
        if unit.id in self.connections:
            del self.connections[unit.id]
        if unit.id in self.unit_map:
            del self.unit_map[unit.id]
        for k in self.connections:
            if unit.id in self.connections[k]:
                del self.connections[k][unit.id]
                self.reverse_connections.get(unit.id, set()).discard(k)
        self._update_global_counts()

        # æŠŠè¿™ä¸ªè¢«åˆ å•å…ƒå½“æˆâ€œfromâ€çš„æ‰€æœ‰åå‘ç´¢å¼•éƒ½æ¸…ç†æ‰
        for to_id, from_set in self.reverse_connections.items():
            if unit.id in from_set:
                from_set.discard(unit.id)
        # ç„¶åå†æŠŠè‡ªå·±é‚£æ¡ key åˆ æ‰
        self.reverse_connections.pop(unit.id, None)

    def connect(self, from_unit: CogUnit, to_unit: CogUnit):
        # ä»…å…è®¸åˆæ³•ç»“æ„è¿æ¥
        valid_links = {
            "sensor": ["processor"],
            "processor": ["processor", "emitter"],
            "emitter": []
        }
        from_role = from_unit.get_role()
        to_role = to_unit.get_role()

        if to_role not in valid_links.get(from_role, []):
            logger.debug(f"[éæ³•è¿æ¥é˜»æ­¢] ä¸å…è®¸ {from_role} â†’ {to_role}ï¼Œè·³è¿‡è¿æ¥ {from_unit.id} â†’ {to_unit.id}")
            return  # ğŸš« é˜»æ­¢éæ³•è¿æ¥

        if from_unit.id not in self.connections:
            self.connections[from_unit.id] = {}  # to_id â†’ strength

        if to_unit.id in self.connections[from_unit.id]:
            return

        # è¶…è¿‡ä¸Šé™æ—¶ï¼Œç§»é™¤ strength æœ€å¼±çš„è¿æ¥
        if len(self.connections[from_unit.id]) >= MAX_CONNECTIONS:
            weakest_id = min(
                self.connections[from_unit.id],
                key=lambda uid: self.connections[from_unit.id][uid]
            )
            del self.connections[from_unit.id][weakest_id]
            self.reverse_connections.get(weakest_id, set()).discard(from_unit.id)

            logger.debug(f"[è¿æ¥æ›¿æ¢] {from_unit.id} ç§»é™¤æœ€å¼±è¿æ¥ {weakest_id}")

        # å»ºç«‹æ–°è¿æ¥ï¼Œåˆå§‹æƒé‡ä¸º 1.0
        self.connections[from_unit.id][to_unit.id] = 1.0
        strength = self.connections[from_unit.id][to_unit.id]
        logger.debug(f"[è¿æ¥å»ºç«‹] {from_unit.id} â†’ {to_unit.id} (strength={strength:.2f})")
        # åŒæ­¥ç»´æŠ¤åå‘ç´¢å¼•
        self.reverse_connections.setdefault(to_unit.id, set()).add(from_unit.id)

    def total_energy(self):
        return sum(unit.energy for unit in self.units if unit.age < 240)

    # ========== ç»´åº¦é€‚é…è¾…åŠ© ==========
    def _goal_dim(self) -> int:
        """è¿”å›å½“å‰ç›®æ ‡å‘é‡é•¿åº¦ (= env_sizeÂ²)"""
        return self.env_size * self.env_size

    def _align_to_goal_dim(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        æŠŠä»»æ„é•¿åº¦çš„å‘é‡å¯¹é½åˆ° env_sizeÂ²ï¼š
        - å¦‚æœæ°å¥½ç›¸ç­‰ â†’ åŸæ ·
        - å¦‚æœèƒ½æ•´é™¤ â†’ reshape(k, goal_dim) åæ±‚å‡å€¼ â†’ goal_dim
          ï¼ˆé»˜è®¤ 4 é€šé“æ—¶ç›¸å½“äºæŠŠ 4 ä¸ªé€šé“å‹ç¼©æˆ 1 é€šé“ï¼‰
        - å¦‚æœæ›´é•¿ä½†æ— æ³•æ•´é™¤ â†’ æˆªæ–­åˆ°å‰ goal_dim
        - å¦‚æœæ›´çŸ­ â†’ å³ä¾§è¡¥é›¶
        """
        goal_dim = self._goal_dim()
        length = tensor.shape[-1]

        if length == goal_dim:
            return tensor

        if length % goal_dim == 0:
            k = length // goal_dim
            return tensor.reshape(-1, k, goal_dim).mean(dim=1).squeeze(0)

        if length > goal_dim:            # æˆªæ–­
            return tensor[..., :goal_dim]

        # length < goal_dim  â†’ å³è¡¥é›¶
        pad = (0, goal_dim - length)
        return torch.nn.functional.pad(tensor, pad)

    # ------------------------------------------------------------------
    # ğŸ†• ä¾›å¼ºåŒ–å­¦ä¹ è°ƒç”¨çš„ç®€åŒ–æ¥å£
    def reset_state(self):
        """
        æ¯ä¸ª episode å¼€å§‹æ—¶è°ƒç”¨ã€‚è¿™é‡Œåªæ¸…é›¶ç¬æ—¶è®¡æ•°å™¨ï¼Œ
        ä¸é‡ç½®èƒ½é‡ / age ç­‰é•¿æœŸæŒ‡æ ‡ã€‚
        """
        for u in self.units:
            u.call_history.clear()
            u.inactive_steps = 0
        # â€”â€” è‹¥è¦æ¯ä¸ª episode ä»å¤´å¼€å§‹ï¼Œè¯·å–æ¶ˆä¸‹é¢æ³¨é‡Š â€”â€”
        # self.current_step = 0
        # self.energy_pool   = self.initial_energy_pool  # åœ¨ __init__ ä¸­ä¿å­˜åˆå§‹å€¼
        # self.connections   = {u.id: {} for u in self.units}
        # self.reverse_connections = {u.id: set() for u in self.units}
        # # å¦‚æœ‰å¿…è¦ï¼Œä¹Ÿé‡ç½®æ¯ä¸ªå•å…ƒçš„ age / energy / subsystem_id ç­‰
        # for u in self.units:
        # u.age = 0
        # u.energy = u.initial_energy  # éœ€åœ¨ CogUnit ä¸­ä¿å­˜åˆå§‹èƒ½é‡



    def sensor_forward(self, env_state_np):
        """
        Args:
            env_state_np : np.ndarray æˆ– torch.Tensor (size=N)
        Returns:
            torch.Tensor (size = env_state_np.size) â€”â€” ä½œä¸º sensor è¾“å‡º
        """
        dev = self.device  # â† ç»Ÿä¸€ç›®æ ‡è®¾å¤‡
        x = torch.as_tensor(env_state_np, dtype=torch.float32, device=dev).view(-1)
        if x.numel() < self.processor_hidden_size:
            pad = (0, self.processor_hidden_size - x.numel())
            x = torch.nn.functional.pad(x, pad)
        else:
            x = x[: self.processor_hidden_size]
        sensors = [u for u in self.units if u.get_role() == "sensor"]
        if sensors:
            outs = []
            for s in sensors:
                s.update(x.unsqueeze(0))
                outs.append(s.get_output().view(-1))
            return torch.stack(outs).mean(dim=0)
        return x.to(dev)

    def processor_forward(self, sensor_out):
        """
        Args:
            sensor_out : torch.Tensor 1-D
        Returns:
            torch.Tensor (size = self.processor_hidden_size)
        """
        dev = self.device  # â† ç»Ÿä¸€ç›®æ ‡è®¾å¤‡
        sensor_out = sensor_out.to(dev)
        processors = [u for u in self.units if u.get_role() == "processor"]
        if processors:
            inp = sensor_out.unsqueeze(0)  # (1,D)
            outs = []
            for p in processors:
                p.update(inp)
                outs.append(p.get_output().view(-1))
            merged = torch.stack(outs).mean(dim=0)
        else:
            merged = sensor_out

        # â€”â€” ç»Ÿä¸€åˆ° processor_hidden_size â€”â€”
        if merged.numel() < self.processor_hidden_size:
            pad = (0, self.processor_hidden_size - merged.numel())
            merged = torch.nn.functional.pad(merged, pad)
        else:
            merged = merged[: self.processor_hidden_size]
        return merged.to(dev)

    def emitter_forward(self, processor_out):
        """
        æŠŠ processor_out é€’ç»™æ‰€æœ‰ emitter åšä¸€æ¬¡æ›´æ–°ï¼›
        ä¸è¦æ±‚è¿”å›å€¼ï¼ˆè‹¥ä½ æƒ³è°ƒè¯•ï¼Œå¯ return å¹³å‡è¾“å‡ºï¼‰ã€‚
        """
        dev = self.device  # â† ç»Ÿä¸€ç›®æ ‡è®¾å¤‡
        processor_out = processor_out.to(dev)
        emitters = [u for u in self.units if u.get_role() == "emitter"]
        if emitters:
            inp = processor_out.unsqueeze(0)  # (1,D)
            for e in emitters:
                e.update(inp)

    def merge_redundant_units(self):
        merged_pairs = set()
        new_units = []

        for i, u1 in enumerate(self.units):
            for j in range(i + 1, len(self.units)):
                u2 = self.units[j]

                # è·³è¿‡å·²æ ‡è®°
                if u1.id in merged_pairs or u2.id in merged_pairs:
                    continue

                # å¿…é¡»æ˜¯ processor æˆ– emitter
                if u1.get_role() != u2.get_role():
                    continue

                # è·ç¦»åˆ¤æ–­
                def euclidean(p1, p2):
                    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5

                dist = euclidean(u1.get_position(), u2.get_position())
                if dist > 3.0:
                    continue

                # è¾“å‡ºç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆcosine similarityï¼‰
                # === è¾“å‡ºç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆcosine similarityï¼‰===
                output1 = u1.get_output().squeeze(0)
                output2 = u2.get_output().squeeze(0)

                # ğŸ”¥ è‡ªåŠ¨è¡¥é›¶åˆ°å½“å‰ç¯å¢ƒé¢„æœŸå°ºå¯¸
                target_dim = max(output1.shape[0], output2.shape[0])

                if output1.shape[0] < target_dim:
                    padding = (0, target_dim - output1.shape[0])
                    output1 = torch.nn.functional.pad(output1, padding, value=0)

                if output2.shape[0] < target_dim:
                    padding = (0, target_dim - output2.shape[0])
                    output2 = torch.nn.functional.pad(output2, padding, value=0)

                sim = F.cosine_similarity(output1, output2, dim=0).item()

                if sim < 0.95:
                    continue

                # âœ… æ»¡è¶³æ¡ä»¶ï¼Œæ‰§è¡Œåˆå¹¶
                logger.info(f"[åˆå¹¶è§¦å‘] {u1.id} å’Œ {u2.id} åˆå¹¶ä¸ºæ–°å•å…ƒ")

                merged = CogUnit(role=u1.get_role())
                merged.position = (
                    (u1.get_position()[0] + u2.get_position()[0]) // 2,
                    (u1.get_position()[1] + u2.get_position()[1]) // 2
                )
                merged.state = (u1.state + u2.state) / 2
                merged.age = int((u1.age + u2.age) / 2)
                merged.energy = u1.energy + u2.energy + 0.1  # å¥–åŠ±åˆå¹¶èƒ½é‡
                merged.last_output = (u1.last_output + u2.last_output) / 2

                # åŠ å…¥æ–°å•å…ƒ
                new_units.append(merged)
                merged_pairs.update({u1.id, u2.id})

                # é‡å®šå‘è¿æ¥ï¼š
                for from_id, to_dict in self.connections.items():
                    if u1.id in to_dict or u2.id in to_dict:
                        if from_id in self.unit_map:  # é˜²æ­¢ä¸Šæ¸¸å·²ç»è¢«åˆå¹¶åˆ é™¤
                            self.connect(self.unit_map[from_id], merged)

                for to_id in self.connections.get(u1.id, {}):
                    if to_id in self.unit_map:  # âœ… é˜²æ­¢è¿æ¥åˆ°å·²è¢«åˆ é™¤çš„å•å…ƒ
                        self.connect(merged, self.unit_map[to_id])
                        logger.debug(f"[è¿æ¥é‡å®šå‘] {merged.id} â†’ {to_id}ï¼ˆç»§æ‰¿è‡ª {u1.id}ï¼‰")

                for to_id in self.connections.get(u2.id, {}):
                    if to_id in self.unit_map:
                        self.connect(merged, self.unit_map[to_id])
                        logger.debug(f"[è¿æ¥é‡å®šå‘] {merged.id} â†’ {to_id}ï¼ˆç»§æ‰¿è‡ª {u2.id}ï¼‰")

        # æ‰§è¡Œåˆ é™¤ & æ·»åŠ 
        for uid in merged_pairs:
            if uid in self.unit_map:
                logger.info(f"[åˆå¹¶åˆ é™¤] {uid}")
                self.remove_unit(self.unit_map[uid])

        for u in new_units:
            self.add_unit(u)
        self._update_global_counts()

    def restructure_common_subgraphs(self):
        """
        æ£€æŸ¥å¹¶é‡æ„é«˜åº¦å…±ç°ã€è¾“å‡ºç›¸ä¼¼çš„ processor â†’ emitter å­å›¾ç»“æ„ã€‚
        å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªæ–°å­å›¾ï¼šnew_processor â†’ new_emitter
        """
        candidates = []

        # éå†æ‰€æœ‰ processor â†’ emitter è¿æ¥
        for u1 in self.units:
            if u1.get_role() != "processor":
                continue
            for eid in self.connections.get(u1.id, {}):
                if eid not in self.unit_map:
                    continue
                u2 = self.unit_map[eid]
                if u2.get_role() != "emitter":
                    continue
                candidates.append((u1, u2))

        # æ£€æŸ¥æ¯å¯¹å­å›¾æ˜¯å¦æ»¡è¶³å…±ç°ä¸è¾“å‡ºç›¸ä¼¼
        for i in range(len(candidates)):
            for j in range(i + 1, len(candidates)):
                p1, e1 = candidates[i]
                p2, e2 = candidates[j]

                if p1.id == p2.id or e1.id == e2.id:
                    continue

                # 1. æ£€æŸ¥å…±ç°ï¼ˆæœ€è¿‘ 5 æ­¥è°ƒç”¨é¢‘ç‡éƒ½ä¸ä¸º 0ï¼‰
                if min(p1.call_history[-3:], default=0) == 0 or min(p2.call_history[-3:], default=0) == 0:
                    continue

                # 2. æ£€æŸ¥è¾“å‡ºç›¸ä¼¼æ€§
                import torch.nn.functional as F
                out1 = p1.get_output()
                out2 = p2.get_output()

                # ğŸ”¥ è‡ªåŠ¨è¡¥é›¶åˆ°å½“å‰ç¯å¢ƒ target_dim
                target_dim = self.env_size * self.env_size * INPUT_CHANNELS

                if out1.shape[-1] < target_dim:
                    padding = (0, target_dim - out1.shape[-1])
                    out1 = torch.nn.functional.pad(out1, padding, value=0)

                if out2.shape[-1] < target_dim:
                    padding = (0, target_dim - out2.shape[-1])
                    out2 = torch.nn.functional.pad(out2, padding, value=0)

                sim_p = F.cosine_similarity(out1, out2, dim=-1).item()

                # ç»Ÿä¸€è®¡ç®— processor è¾“å‡ºä¸ emitter è¾“å‡ºçš„ç›¸ä¼¼æ€§ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´
                out1 = p1.get_output()
                out2 = p2.get_output()
                out_e1 = e1.get_output()
                out_e2 = e2.get_output()

                max_dim = max(
                    out1.shape[-1], out2.shape[-1],
                    out_e1.shape[-1], out_e2.shape[-1],
                    self.env_size * self.env_size * INPUT_CHANNELS
                )

                def pad_to(tensor, target_dim):
                    if tensor.shape[-1] < target_dim:
                        padding = (0, target_dim - tensor.shape[-1])
                        return F.pad(tensor, padding, value=0)
                    return tensor

                out1 = pad_to(out1, max_dim)
                out2 = pad_to(out2, max_dim)
                out_e1 = pad_to(out_e1, max_dim)
                out_e2 = pad_to(out_e2, max_dim)

                sim_p = F.cosine_similarity(out1, out2, dim=-1).item()
                sim_e = F.cosine_similarity(out_e1, out_e2, dim=-1).item()

                if sim_p > 0.95 and sim_e > 0.95:
                    # âœ… æ»¡è¶³é‡æ„æ¡ä»¶
                    logger.info(f"[é‡æ„è§¦å‘] å­å›¾ ({p1.id}â†’{e1.id}) ä¸ ({p2.id}â†’{e2.id}) ç›¸ä¼¼ï¼Œå¼€å§‹é‡æ„")

                    # åˆ›å»ºæ–°å•å…ƒ
                    new_p = CogUnit(role="processor")
                    new_e = CogUnit(role="emitter")
                    new_p.state = (p1.state + p2.state) / 2
                    new_p.last_output = (p1.last_output + p2.last_output) / 2
                    new_e.state = (e1.state + e2.state) / 2
                    new_e.last_output = (e1.last_output + e2.last_output) / 2

                    new_p.energy = p1.energy + p2.energy + 0.1
                    new_e.energy = e1.energy + e2.energy + 0.1

                    # æ’å…¥æ–°å•å…ƒ
                    self.add_unit(new_p)
                    self.add_unit(new_e)
                    self.connect(new_p, new_e)

                    # å°†æ‰€æœ‰è¿æ¥åˆ° p1 / p2 çš„ä¸Šæ¸¸æŒ‡å‘ new_p
                    for uid in list(self.unit_map):
                        if p1.id in self.connections.get(uid, {}) or p2.id in self.connections.get(uid, {}):
                            self.connect(self.unit_map[uid], new_p)

                    # åˆ é™¤åŸå­å›¾
                    logger.info(f"[é‡æ„åˆ é™¤] åˆ é™¤åŸå­å›¾ ({p1.id}â†’{e1.id}) å’Œ ({p2.id}â†’{e2.id})")
                    self.remove_unit(p1)
                    self.remove_unit(p2)
                    self.remove_unit(e1)
                    self.remove_unit(e2)

                    # é‡æ„åæ›´æ–°å…¨å±€è®¡æ•°
                    self._update_global_counts()
                    return  # æ¯è½®åªé‡æ„ä¸€ç»„â€¦

    def assign_subsystems(self, min_size=3, max_size=10):
        """
        è‡ªåŠ¨å‘ç°å±€éƒ¨é«˜å¯†åº¦è¿æ¥åŒºåŸŸï¼Œæ ‡è®°ä¸ºå­ç³»ç»Ÿ
        """
        visited = set()
        subsystem_count = 0

        for unit in self.units:
            if unit.id in visited:
                continue

            # ä»¥å½“å‰unitä¸ºèµ·ç‚¹ï¼Œåšå±€éƒ¨DFS
            cluster = self._dfs_collect_cluster(unit, max_depth=2)

            if min_size <= len(cluster) <= max_size:
                subsystem_id = f"subsys-{subsystem_count}"
                for u in cluster:
                    u.subsystem_id = subsystem_id
                subsystem_count += 1
                logger.info(f"[å­ç³»ç»Ÿç”Ÿæˆ] æ–°å­ç³»ç»Ÿ {subsystem_id}ï¼ŒåŒ…å« {len(cluster)} ä¸ªå•å…ƒ")
                visited.update(u.id for u in cluster)

    def _dfs_collect_cluster(self, start_unit, max_depth=2):
        """
        è¾…åŠ©ï¼šæ·±åº¦ä¼˜å…ˆæœç´¢ï¼Œæ‰¾å‡ºå±€éƒ¨è¿æ¥çš„å•å…ƒç¾¤
        """
        cluster = set()
        stack = [(start_unit, 0)]
        while stack:
            unit, depth = stack.pop()
            if depth > max_depth or unit in cluster:
                continue
            cluster.add(unit)
            for neighbor_id in self.connections.get(unit.id, {}):
                neighbor = self.unit_map.get(neighbor_id)
                if neighbor:
                    stack.append((neighbor, depth + 1))

        return list(cluster)

    def prune_connections(self, prune_ratio=0.2, strengthen_ratio=1.5):
        """
        è‡ªåŠ¨å‰ªæ‰ä½æ•ˆè¿æ¥ï¼Œå¼ºåŒ–é«˜æ•ˆè¿æ¥
        :param prune_ratio: å°äºå…¨å±€å¹³å‡è°ƒç”¨é¢‘ç‡ * prune_ratio çš„è¿æ¥ä¼šè¢«å‰ªæ‰
        :param strengthen_ratio: å¤§äºå…¨å±€å¹³å‡è°ƒç”¨é¢‘ç‡ * strengthen_ratio çš„è¿æ¥ä¼šè¢«å¼ºåŒ–
        """
        if not self.connection_usage:
            return

        usage_values = list(self.connection_usage.values())
        avg_usage = sum(usage_values) / len(usage_values)

        to_prune = []
        to_strengthen = []

        for conn, usage in self.connection_usage.items():
            if usage < avg_usage * prune_ratio:
                to_prune.append(conn)
            elif usage > avg_usage * strengthen_ratio:
                to_strengthen.append(conn)

        # å‰ªæ‰ä½æ•ˆè¿æ¥
        for conn in to_prune:
            from_unit, to_unit = conn
            if to_unit in self.connections.get(from_unit, {}):
                del self.connections[from_unit][to_unit]  # âœ… åˆ é™¤ dict çš„ key
                self.reverse_connections.get(to_unit, set()).discard(from_unit.id)

            logger.debug(f"[å‰ªæ] è¿æ¥ {from_unit} â†’ {to_unit} è¢«å‰ªæ‰")
            # ä¹Ÿåˆ æ‰ usageè®°å½•
            if conn in self.connection_usage:
                del self.connection_usage[conn]

        # å¼ºåŒ–é«˜æ•ˆè¿æ¥ï¼ˆå¯é€‰ï¼šæ¯”å¦‚å¢åŠ èƒ½é‡ä¼ é€’æƒé‡ç­‰ï¼‰
        for conn in to_strengthen:
            # ç®€å•æ‰“å°æ ‡è®°ï¼Œå¯ä»¥åç»­åŠ çœŸå®æƒé‡ç³»ç»Ÿ
            logger.debug(f"[å¼ºåŒ–] è¿æ¥ {conn[0]} â†’ {conn[1]} è¢«å¼ºåŒ–")

        logger.info(f"[å‰ªæ] å‰ªæ‰ {len(to_prune)} æ¡å¼±è¿æ¥ï¼Œå¼ºåŒ– {len(to_strengthen)} æ¡å¼ºè¿æ¥")

    def auto_connect(self):
        def euclidean(p1, p2):
            return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5

        for unit in self.units:
            role = unit.get_role()

            if role == "processor":
                # processor å¯»æ‰¾ä¸‹æ¸¸è¿æ¥å¯¹è±¡ï¼ˆprocessor æˆ– emitterï¼‰
                target_roles = ["processor", "emitter"]
            elif role == "emitter":
                # emitter ä¸åº”è¯¥ä¸»åŠ¨è¿æ¥ï¼ˆskipï¼‰
                continue
            else:
                continue  # sensor ä¸å‚ä¸

            current_connections = self.connections[unit.id]

            if len(current_connections) < 2:
                u_pos = unit.get_position()
                candidates = [
                    u for u in self.units
                    if u.id != unit.id and u.get_role() in target_roles
                       and abs(u.input_size - unit.input_size) <= 100
                       and u.id not in current_connections
                       and euclidean(u.get_position(), u_pos) < 3
                ]

                if not candidates:
                    # æ²¡æœ‰è¿‘é‚» â†’ å…¨å±€æœç´¢
                    candidates = [
                        u for u in self.units
                        if u.id != unit.id and u.get_role() in target_roles
                           and u.id not in current_connections
                    ]

                if candidates:
                    def connection_strength(u):
                        incoming_count = sum(u.id in self.connections.get(fid, {}) for fid in self.unit_map)
                        return u.energy + incoming_count * 0.1

                    candidates.sort(key=connection_strength, reverse=True)

                    for target in candidates:
                        if target.id not in current_connections:
                            prev_conn_count = len(self.connections[unit.id])
                            self.connect(unit, target)
                            if len(self.connections[unit.id]) > prev_conn_count:
                                logger.debug(f"[æ–°è¿æ¥] {unit.id} â†’ {target.id}")
                                break  # âœ… æˆåŠŸå»ºç«‹è¿æ¥å°±è·³å‡º

        # === éšæœºçªå˜è¿æ¥ï¼ˆåªå…è®¸ processor å‘èµ·ï¼‰ ===
        if random.random() < 0.1:
            from_candidates = [u for u in self.units if u.get_role() == "processor"]
            to_candidates = [u for u in self.units if u.get_role() in ["processor", "emitter"]]

            if from_candidates and to_candidates:
                from_unit = random.choice(from_candidates)
                to_unit = random.choice(to_candidates)

                if to_unit.id not in self.connections.get(from_unit.id, {}):
                    self.connect(from_unit, to_unit)
                    logger.debug(f"[çªå˜è¿æ¥] {from_unit.id} â†’ {to_unit.id}")

    # === åˆ†åŒ–æœºåˆ¶ï¼šç»“æ„å¤±è¡¡æ—¶çš„è§’è‰²è°ƒæ•´ ===
    def rebalance_cell_types(self):
        from collections import Counter
        total = len(self.units)
        if total < 15:
            return  # å¤ªå°å…ˆè‡ªç”±ç”Ÿé•¿

        # åŠ¨æ€è¿Ÿæ»çª—å£  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #   æ€»æ•°   <50   <200   <500   500+
        #   hi    1.50  1.30   1.15   1.08
        #   lo    0.50  0.70   0.85   0.92
        if total < 50:
            hi, lo = 1.30, 0.60
        elif total < 200:
            hi, lo = 1.08, 0.92
        elif total < 500:
            hi, lo = 1.05, 0.95
        else:
            hi, lo = 1.03, 0.97

        # Î” å®¹å·®ï¼ˆè‡³å°‘ç›¸å·® Î”_cell æ‰ç®—â€œçœŸçš„å¤šï¼å°‘â€ï¼‰
        delta_cell = max(1, int(total * TOL_FRAC))

        # æœ¬è½®æœ€å¤šè½¬æ¢
        max_conv = max(1, int(total * MAX_CONV_FRAC))
        conv_done = 0

        def pick_weakest(units):
            return min(units, key=lambda u: (u.energy,
                                             getattr(u, "avg_recent_calls", 0.0)))

        while conv_done < max_conv:
            # â”€â”€ é‡æ–°è®¡æ•°
            young_units = [u for u in self.units if u.age < 240]
            cnt = Counter(u.get_role() for u in young_units)
            s_cnt = cnt.get("sensor", 0)
            p_cnt = cnt.get("processor", 0)
            e_cnt = cnt.get("emitter", 0)

            desired = {
                "sensor": total * IDEAL_RATIO["sensor"] / DENOM,
                "processor": total * IDEAL_RATIO["processor"] / DENOM,
                "emitter": total * IDEAL_RATIO["emitter"] / DENOM,
            }

            # ratio & diff
            ratio = {
                "sensor": s_cnt / (desired["sensor"] or 1),
                "processor": p_cnt / (desired["processor"] or 1),
                "emitter": e_cnt / (desired["emitter"] or 1),
            }
            diff = {
                "sensor": s_cnt - desired["sensor"],
                "processor": p_cnt - desired["processor"],
                "emitter": e_cnt - desired["emitter"],
            }

            # 1) æ»¡è¶³ ratio>hi ä¸” diffâ‰¥Î” æ‰ç®—â€œoverâ€   2) ratio<lo ä¸” diffâ‰¤-Î” ç®—â€œunderâ€
            overs = [r for r in ratio if ratio[r] > hi and diff[r] >= delta_cell]
            unders = [r for r in ratio if ratio[r] < lo and diff[r] <= -delta_cell]

            if not overs or not unders:
                break  # è½å…¥è¿Ÿæ»å¸¦ or Î” å¤ªå°ï¼Œç»“æŸ

            # é€‰æœ€è¿‡é‡ & æœ€ä¸è¶³
            giver_role = max(overs, key=lambda r: diff[r])  # diff æœ€å¤§
            receiver_role = min(unders, key=lambda r: diff[r])  # diff æœ€å°(è´Ÿæ•°)

            # å– giver_role æœ€å¼±è€…
            cand = [u for u in self.units if u.get_role() == giver_role]
            if not cand:
                break
            unit = pick_weakest(cand)

            # â”€â”€ è½¬åŒ–
            old = unit.get_role()
            unit.role = receiver_role
            unit.age = 0
            unit.energy += 0.2
            unit.gene[f"{receiver_role}_bias"] = 1.0
            logger.info(f"[å¹³è¡¡] {old}â†’{receiver_role} | step={self.current_step}")

            # æ¸…æ—§è¿ & ç®€æ˜“æ–°è¿
            for uid, out_edges in list(self.connections.items()):
                out_edges.pop(unit.id, None)
                self.connection_usage.pop((uid, unit.id), None)
            self.connections[unit.id] = {}

            if receiver_role == "processor":
                tgt = max((u for u in self.units if u.get_role() == "emitter"),
                          default=None, key=lambda u: u.energy)
                if tgt: self.connect(unit, tgt)
            elif receiver_role == "emitter":
                src = max((u for u in self.units if u.get_role() == "processor"),
                          default=None, key=lambda u: u.energy)
                if src: self.connect(src, unit)
            elif receiver_role == "sensor":
                tgt = max((u for u in self.units if u.get_role() == "processor"),
                          default=None, key=lambda u: u.energy)
                if tgt: self.connect(unit, tgt)

            conv_done += 1
            self._update_global_counts()


    def trace_info_paths(self):
        logger.debug(f"[ä¿¡æ¯è·¯å¾„è¿½è¸ª] æ­¥æ•° {self.current_step}")
        for emitter in self.units:
            if emitter.get_role() != "emitter":
                continue

            # è¿½æº¯ä¸Šæ¸¸ processor
            emit_from = [pid for pid in self.unit_map if emitter.id in self.connections.get(pid, {})]
            for pid in emit_from:
                proc_from = [sid for sid in self.unit_map if pid in self.connections.get(sid, {})]
                for sid in proc_from:
                    logger.debug(f"  sensor:{sid} â†’ processor:{pid} â†’ emitter:{emitter.id}")

    def _select_clone_parents(self, pending_by_role):
        """
        ä»å¾…å¤åˆ¶çˆ¶å•å…ƒä¸­ï¼ŒæŒ‰ç…§é…é¢ & èƒ½é‡/æ´»è·ƒåº¦æ’åºæŒ‘å‡ºçœŸæ­£å…è®¸å¤åˆ¶çš„ã€‚
        è‹¥ç»†èƒèƒ½é‡è¶…è¿‡ 3.0ï¼Œå¼ºåˆ¶å…è®¸å¤åˆ¶ï¼Œä¸å—æ¯”ä¾‹é™åˆ¶ã€‚
        """
        total_cells = len(self.units)
        approved = set()

        if total_cells <= 15:
            for lst in pending_by_role.values():
                approved.update(lst)
        else:
            for role, cand in pending_by_role.items():
                if not cand:
                    continue
                young_units = [u for u in self.units if u.role == role and u.age < 240]
                role_count = len(young_units)
                cap = max(1, (2 * role_count) // 5)  # 40%
                cand.sort(key=lambda u: (u.energy, u.avg_recent_calls), reverse=True)
                approved.update(cand[:cap])

        return list(approved)

    def trim_weak_memories(self):
        """ç¯å¢ƒå‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ¸…é™¤æ‰€æœ‰ç»†èƒè®°å¿†æ± ä¸­çš„ä¸€åŠæœ€å¼±è®°å¿†"""
        for unit in self.units:
            if hasattr(unit, "local_memory_pool") and unit.local_memory_pool:
                pool = unit.local_memory_pool
                pool.sort(key=lambda m: m["score"])
                half = len(pool) // 2
                del pool[:half]

    def step(self, input_tensor: torch.Tensor):
        self._update_global_counts()
        self.current_step += 1
        env_state = torch.from_numpy(self.env.get_state()).float().to(self.device).unsqueeze(0)
        goal_tensor = self.task.encode_goal(self.env_size).unsqueeze(0).to(self.device)

        if self.current_step == 10000:
            self.subsystem_competition = True
            logger.info("[è¿›åŒ–] å­ç³»ç»Ÿç«äº‰æœºåˆ¶å·²æ¿€æ´»ï¼ˆSubsystem Competitionï¼‰")

        if self.current_step == 2000:
            for unit in self.units:
                unit.dynamic_aging = True
            logger.info("[è¿›åŒ–] åŠ¨æ€å¯¿å‘½æœºåˆ¶å·²æ¿€æ´»ï¼ˆDynamic Agingï¼‰")



        if self.current_step > 200 and self.current_step % 10 == 0:
            total_cell_energy = self.total_energy()
            pool_energy = self.energy_pool
            total_e = total_cell_energy + pool_energy
            max_e = self.max_total_energy

            if total_e > max_e:
                excess = total_e - max_e
                tiers = [
                    (0.00, 0.15, 0.01),  # è¶…å‡º 0~15% éƒ¨åˆ†æ”¶ 1%
                    (0.15, 0.35, 0.05),  # è¶…å‡º 15~35% éƒ¨åˆ†æ”¶ 5%
                    (0.35, 0.55, 0.10),  # è¶…å‡º 35~55% éƒ¨åˆ†æ”¶ 10%
                    (0.50, float("inf"), 0.50)  # è¶…å‡º 55% éƒ¨åˆ†æ”¶ 50%
                ]

                tax = 0.0
                for lower, upper, rate in tiers:
                    lower_abs = max_e * lower
                    upper_abs = max_e * upper
                    if excess > lower_abs:
                        taxed_amount = min(excess, upper_abs) - lower_abs
                        tax += taxed_amount * rate

                if pool_energy >= tax:
                    self.energy_pool -= tax
                    logger.info(
                        f"[èƒ½é‡ç¨] {self.current_step} æ­¥ï¼šæ€»èƒ½ {total_e:.2f} â†’ ç´¯è¿›ç¨ {tax:.2f}ï¼ˆæ± è¶³å¤Ÿï¼Œå‰©ä½™æ± èƒ½ {self.energy_pool:.2f}ï¼‰")
                else:
                    tax_from_cells = tax - self.energy_pool
                    self.energy_pool = 0.0
                    loss_per_unit = tax_from_cells / max(len(self.units), 1)
                    for unit in self.units:
                        unit.energy -= loss_per_unit
                    logger.info(
                        f"[èƒ½é‡ç¨] {self.current_step} æ­¥ï¼šæ€»èƒ½ {total_e:.2f} â†’ ç¨ {tax:.2f}ï¼Œæ± ä¸è¶³ â†’ ç»†èƒæ¯ä¸ªæ‰£ {loss_per_unit:.4f}")

        # === Curriculum Learning: æ¯500æ­¥æ‰©å±•ä¸€æ¬¡ç¯å¢ƒå¤§å°
        if self.current_step > 0 and self.current_step % 500 == 0:
            old_size = self.env_size
            self.env_size = min(self.env_size + 5, 20)  # æ¯æ¬¡+5ï¼Œæœ€å¤§åˆ°20x20
            self.env = GridEnvironment(size=self.env_size)  # é‡æ–°ç”Ÿæˆç¯å¢ƒ
            self.upscale_old_units(self.env_size * self.env_size * INPUT_CHANNELS)
            self.processor_hidden_size = self.env_size * self.env_size * INPUT_CHANNELS

            new_target = (random.randint(0, self.env_size - 1), random.randint(0, self.env_size - 1))
            self.task = TaskInjector(target_position=new_target)
            self.target_vector = self.task.encode_goal(self.env_size)
            logger.info(
                f"[Curriculumå‡çº§] ç¬¬ {self.current_step} æ­¥ï¼šç¯å¢ƒå¤§å° {old_size}x{old_size} â†’ {self.env_size}x{self.env_size}ï¼Œæ–°ç›®æ ‡ {new_target}")

        if self.current_step > 0 and self.current_step % 1000 == 0:
            old_max = self.max_total_energy
            self.max_total_energy *= 2
            logger.info(f"[èµ„æºæ‰©å±•] ç¬¬ {self.current_step} æ­¥ï¼šMAX_TOTAL_ENERGY {old_max:.1f} â†’ {self.max_total_energy:.1f}")

        # è‹¥å½“å‰æ­¥æ•°éå¸¸æ—©æœŸï¼Œç»™äºˆåŸºç¡€èƒ½é‡è¡¥å¿
        if self.current_step < 10:
            for unit in self.units:
                if unit.get_role() != "sensor":
                    unit.energy += 0.1
                    logger.debug(f"[é¢„çƒ­è¡¥å¿] {unit.id} åˆå§‹é˜¶æ®µè·å¾—èƒ½é‡ +0.1")

        if self.current_step > 0 and self.current_step % 100 == 0:
            old_target = self.target_vector.clone()
            self.target_vector = torch.rand_like(self.target_vector)

            similarity = torch.cosine_similarity(old_target, self.target_vector, dim=0).item()
            logger.info(f"[ç›®æ ‡å˜åŒ–] ç¬¬ {self.current_step} æ­¥ï¼Œtarget_vector æ›´æ–°ï¼ï¼ˆç›¸ä¼¼åº¦ {similarity:.3f}ï¼‰")

        if self.current_step > 0 and self.current_step % 100 == 0:
            self.prune_connections()

        if self.current_step > 0 and self.current_step % 300 == 0:
            self.assign_subsystems()

        if hasattr(self, "subsystem_competition") and self.subsystem_competition:
            if self.current_step % 100 == 0:
                subsystem_energies = {}
                for unit in self.units:
                    if unit.subsystem_id:
                        subsystem_energies.setdefault(unit.subsystem_id, 0)
                        subsystem_energies[unit.subsystem_id] += unit.energy

                if len(subsystem_energies) >= 5:  # è‡³å°‘5ä¸ªå­ç³»ç»Ÿæ‰ç«äº‰
                    weakest = min(subsystem_energies, key=lambda x: subsystem_energies[x])
                    logger.info(f"[å­ç³»ç»Ÿç«äº‰] æ·˜æ±°èƒ½é‡æœ€å¼±çš„å­ç³»ç»Ÿ {weakest}")

                    # åˆ é™¤å¼±å­ç³»ç»Ÿçš„æ‰€æœ‰å•å…ƒ
                    self.units = [u for u in self.units if u.subsystem_id != weakest]
                    self.unit_map = {u.id: u for u in self.units}
                    self.connections = {u.id: {} for u in self.units}


        if self.current_step > 2000 and self.current_step % 40 == 0:
            total = len(self.units)
            max_elites = max(1, int(total * 0.08))  # æœ€å¤š8%

            # æ”¶é›†æ‰€æœ‰æœ‰è®°å¿†çš„åˆ†æ•°ï¼Œç”¨äºè®¡ç®—é˜ˆå€¼
            all_scores = [
                u.local_memory_pool[-1]["score"]
                for u in self.units
                if len(u.local_memory_pool) >= 1
            ]
            score_threshold = np.percentile(all_scores, 90)

            candidates = []
            for u in self.units:
                # æ¡ä»¶1ï¼šè‡³å°‘5æ¬¡è®°å¿†
                if len(u.local_memory_pool) < 5:
                    continue
                last_score = u.local_memory_pool[-1]["score"]
                # æ¡ä»¶2ï¼šé«˜åˆ†é—¨æ§›
                if last_score < score_threshold:
                    continue
                # æ¡ä»¶3ï¼šæ´»è·ƒåº¦
                if getattr(u, "avg_recent_calls", 0) < 2.0:
                    continue

                # æ¡ä»¶4ï¼šè¾“å‡ºè´¨é‡ role-specific
                # å…ˆä» local_memory_pool æœ€è¿‘å‡ æ¡é‡Œé‡ç®— quality
                hist = [m["output"].view(-1) for m in u.local_memory_pool[-5:]]
                # å¯¹é½
                max_len = max(t.numel() for t in hist)
                aligned = [t if t.numel() == max_len else torch.nn.functional.pad(t, (0, max_len - t.numel())) for t in
                           hist]
                diffs = [(aligned[i] - aligned[i + 1]).norm().item() for i in range(len(aligned) - 1)]
                if u.role == "processor":
                    diversity = sum(diffs) / len(diffs)
                    if diversity < 0.1:
                        continue
                elif u.role == "sensor":
                    variation = torch.var(torch.stack(aligned), dim=0).mean().item()
                    if variation < 0.05:
                        continue
                elif u.role == "emitter":
                    avg_diff = sum(diffs) / len(diffs)
                    stability = 1.0 if 0.01 < avg_diff < 0.5 else 0.0
                    if stability < 1.0:
                        continue

                # å…¨éƒ¨é€šè¿‡ï¼ŒåŠ å…¥å€™é€‰
                candidates.append((u, last_score))

            # æŒ‰åˆ†æ•°é™åºé€‰ top K
            elites = [u for u, _ in sorted(candidates, key=lambda x: x[1], reverse=True)[:max_elites]]

            # é‡ç½®æ—§æ ‡è®° & æ ‡æ–°ç²¾è‹±
            for u in self.units:
                u.is_elite = False
            # æ ‡è®°æ–°ç²¾è‹± & é‡ç½®å¹´é¾„
            for u in elites:
                u.is_elite = True
                u.age = 0  # â† å…³é”®ï¼šæ¸…é›¶å¹´é¾„ï¼Œè®©å®ƒä»å¤´å¼€å§‹ï¼Œé¿å…è¿›å…¥è€åŒ–æ­»äº¡çª—å£

        # æ¢å¤å…¨å±€è®¡æ•°æ›´æ–°ï¼Œé¿å… should_split æ‹¿åˆ°è¿‡æ—¶å€¼


        new_units = []  # æ–°ç”Ÿæˆçš„å•å…ƒï¼ˆå¤åˆ¶ï¼‰
        pending = {"sensor": [], "processor": [], "emitter": []}  # NEW: å¾…å¤åˆ¶çˆ¶å•å…ƒ
        output_buffer = {}  # ç¼“å­˜æ¯ä¸ªå•å…ƒçš„è¾“å‡º {unit_id: output_tensor}

        # === ç³»ç»Ÿæ€»èƒ½é‡é™åˆ¶ï¼Œä¿æŠ¤ clone ===
        allow_clone = self.total_energy() < self.max_total_energy

        # === ç¬¬ä¸€é˜¶æ®µï¼šå•å…ƒæ›´æ–°å¤„ç† ===
        # ç»Ÿè®¡å½“å‰ emitter æ•°é‡
        emitter_count = sum(1 for u in self.units if u.get_role() == "emitter")

        for unit in self.units:
            unit.global_emitter_count = emitter_count


        for unit in self.units[:]:

            unit_input = torch.cat([env_state, goal_tensor], dim=1)

            # å¦‚æœè¯¥å•å…ƒæœ‰ä¸Šæ¸¸è¿æ¥ï¼ˆè¢«å…¶ä»–å•å…ƒæŒ‡å‘ï¼‰
            # O(1) åå‘æŸ¥æ‰¾æ‰€æœ‰è°ƒç”¨è¿‡æˆ‘çš„
            # 1. æŠŠ reverse_connections é‡Œ stale çš„ uid ä¸¢æ‰ï¼Œå‰©ä¸‹æ‰æ˜¯çœŸæ­£æœ‰æ•ˆçš„ incoming
            raw = list(self.reverse_connections.get(unit.id, set()))
            incoming = []
            for uid in raw:
                # å…ˆæ£€æŸ¥ï¼šuid è¿˜åœ¨ unit_map é‡Œï¼Ÿuidâ†’unit.id è¿™æ¡è¿è¾¹è¿˜çœŸåœ¨ connections é‡Œï¼Ÿ
                if uid in self.unit_map and unit.id in self.connections.get(uid, {}):
                    incoming.append(uid)
                    # æ›´æ–° usage & strength
                    self.connection_usage[(uid, unit.id)] = self.current_step
                    self.connections[uid][unit.id] = min(self.connections[uid][unit.id], 5.0)
                else:
                    # è¦ä¹ˆå•å…ƒè¢«åˆ äº†ï¼Œè¦ä¹ˆè¿è¾¹è¢«å‰ªäº† â€”â€” é¡ºä¾¿æ¸…ç†åå‘ç´¢å¼•
                    self.reverse_connections[unit.id].discard(uid)

            if unit.get_role() == "sensor":
                # åŒæ ·ä½¿ç”¨ dim=1 æ‹¼æ¥ï¼Œå†åŠ ä¸€ä¸ª batch ç»´åº¦
                unit_input = torch.cat([env_state, goal_tensor], dim=1)

            elif incoming:
                weighted_outputs = []
                total_weight = 0.0
                for uid in incoming:
                    strength = self.connections[uid][unit.id]
                    output = self.unit_map[uid].get_output().squeeze(0)  # ç»Ÿä¸€ä¸º [8]
                    target_len = self.processor_hidden_size
                    if output.shape[0] != target_len:
                        padding = (0, target_len - output.shape[0])

                        output = torch.nn.functional.pad(output, padding, value=0)

                    weighted_outputs.append(output * strength)
                    total_weight += strength

                if total_weight > 0:
                    unit_input = torch.stack(weighted_outputs).sum(dim=0, keepdim=True) / total_weight
                else:
                    unit_input = torch.zeros_like(input_tensor).unsqueeze(0)
            else:
            # å¼ºåˆ¶ä½¿ç”¨é›¶è¾“å…¥è§¦å‘æ›´æ–°ï¼Œé¿å…å› æ— è¾“å…¥æ°¸è¿œä¸æ›´æ–°
                unit_input = torch.zeros(unit.input_size).unsqueeze(0)
                logger.debug(f"[é›¶è¾“å…¥] {unit.id} æ— ä¸Šæ¸¸è¿æ¥ï¼Œä½¿ç”¨é›¶è¾“å…¥æ›´æ–°")

            # æ‰§è¡Œå•å…ƒçš„æ›´æ–°é€»è¾‘
            # === ç»Ÿè®¡è°ƒç”¨é¢‘ç‡ï¼ˆè¿™é‡Œå¯ä»¥æ›´ç²¾ç»†ï¼Œæ¯”å¦‚ sliding windowï¼‰===
            # O(1) æŸ¥ self.reverse_connections
            incoming = self.reverse_connections.get(unit.id, ())
            unit.recent_calls = len(incoming)
            unit.connection_count = len(self.connections.get(unit.id, {}))  # ç›´æ¥ä¸‹æ¸¸è¿æ¥æ•°é‡
            # æ›´æ–°è°ƒç”¨å†å²è®°å½•
            unit.call_history.append(unit.recent_calls)
            if len(unit.call_history) > unit.call_window:
                unit.call_history.pop(0)

            # è®¡ç®—å¹³å‡è°ƒç”¨é¢‘ç‡
            unit.avg_recent_calls = sum(unit.call_history) / len(unit.call_history)

            if unit.recent_calls == 0:
                unit.inactive_steps += 1
            else:
                unit.inactive_steps = 0  # é‡ç½®

            unit.current_step = self.current_step
            # === ç»Ÿä¸€åŠ¨æ€èƒ½é‡æ¶ˆè€— ===
            var = torch.var(unit_input).item()
            freq = unit.recent_calls
            conn = unit.connection_count
            call_density = freq / (conn + 1)
            conn_strength_sum = sum(self.connections.get(unit.id, {}).values())

            # ç»Ÿä¸€ä»£è°¢æ¨¡å‹ï¼ˆå¯è°ƒæƒé‡ï¼‰
            dim_scale = 50.0 / unit.input_size  # â† 50 æ˜¯æœ€åˆç¯å¢ƒ(5Ã—5Ã—2)çš„åŸºå‡†
            bias_factor = 1.0
            if unit.role == "sensor":
                bias_factor = unit.gene.get("sensor_bias", 1.0)
            elif unit.role == "processor":
                bias_factor = unit.gene.get("processor_bias", 1.0)
            elif unit.role == "emitter":
                bias_factor = unit.gene.get("emitter_bias", 1.0)

            step_factor = 1.0 + 0.0005 * max(0, self.current_step - 500)
            unit_factor = 1.0 + 0.005 * max(0, len(self.units) - 50)

            # ä»£è°¢å…¬å¼åŠ å…¥åŠ¨æ€å› å­
            decay = (var * 0.15 + call_density * 0.04 + conn_strength_sum * 0.02) \
                    * dim_scale * bias_factor * step_factor * unit_factor

            unit.energy -= decay
            unit.energy = max(unit.energy, 0.0)

            logger.debug(
                f"[ä»£è°¢] {unit.id} var={var:.3f}, freq={freq}, conn={conn}, strength_sum={conn_strength_sum:.2f} â†’ -{decay:.3f} èƒ½é‡")

            unit.update(unit_input)


            # âœ… åŠ å¼ºè¿æ¥æƒé‡ï¼ˆä½¿ç”¨æ¬¡æ•°è¶Šå¤šè¶Šå¼ºï¼‰
            for uid in incoming:
                if unit.id in self.connections.get(uid, {}):
                    self.connections[uid][unit.id] *= 1.05  # å¢å¼º
                    self.connections[uid][unit.id] = min(self.connections[uid][unit.id], 5.0)
            output_buffer[unit.id] = unit.get_output()
            logger.debug(str(unit))

            # === åˆ¤æ–­æ˜¯å¦éœ€è¦å¤åˆ¶ ===
            if allow_clone and unit.should_split():
                pending[unit.role].append(unit)   # åªè®°å½•çˆ¶å•å…ƒï¼Œä¸ç«‹å³ clone


            else:
                if not allow_clone:
                    logger.debug(f"[ç³»ç»Ÿä¿æŠ¤] æ€»èƒ½é‡è¿‡é«˜ï¼Œç¦æ­¢ {unit.id} åˆ†è£‚")

            # === åˆ¤æ–­æ˜¯å¦æ­»äº¡ ===
            if unit.should_die():
                logger.debug(f"[æ­»äº¡] {unit.id} è¢«ç§»é™¤")
                self.remove_unit(unit)


        self.auto_connect()
        # === æ­»è¿æ¥æ¸…ç† ===
        if self.current_step % 10 == 0:
            threshold = 50
            for from_id in list(self.connections.keys()):
                for to_id in list(self.connections[from_id].keys()):
                    last_used = self.connection_usage.get((from_id, to_id), -1)
                    if self.current_step - last_used > threshold:
                        del self.connections[from_id][to_id]  # âœ… æ­£ç¡®åˆ é™¤æ–¹å¼
                        self.reverse_connections.get(to_id, set()).discard(from_id)

                        logger.debug(f"[æ­»è¿æ¥æ¸…é™¤] {from_id} â†’ {to_id}")
                        # åˆ é™¤è¿æ¥åï¼Œç»™ from_unit è½»å¾®èƒ½é‡æƒ©ç½š
                        if from_id in self.unit_map:
                            self.unit_map[from_id].energy -= 0.015  # å¯è°ƒå‚æ•°
                            logger.debug(f"[æƒ©ç½š] {from_id} å› è¿æ¥å¤±æ•ˆï¼Œèƒ½é‡ -0.01")

                    else:
                        # âœ… å‰Šå¼±ä»åœ¨ç”¨ä½†è¡¨ç°å·®çš„è¿æ¥
                        self.connections[from_id][to_id] *= 0.95
                        if self.connections[from_id][to_id] < 0.1:
                            del self.connections[from_id][to_id]
                            self.reverse_connections.get(to_id, set()).discard(from_id)
                            logger.debug(f"[è¿æ¥è¡°å‡æ¸…é™¤] {from_id} â†’ {to_id}")

        # ç®€æ˜“ä»»åŠ¡å¥–åŠ±ï¼šå¦‚æœ emitter è¾“å‡ºé è¿‘æŸä¸ªç›®æ ‡å‘é‡ï¼Œåˆ™å‘æ”¾å¥–åŠ±
        target_vector = self.target_vector
        outputs = self.collect_emitter_outputs()
        if outputs is not None:
            avg_output = outputs.mean(dim=0)
            distance = torch.norm(avg_output - target_vector)

            # çº¿æ€§è¡°å‡å¼å¥–åŠ±åˆ†æ•°ï¼ˆè·ç¦» 0â†’å¥–åŠ±æ»¡åˆ†1ï¼Œè·ç¦»3â†’å¥–åŠ±ä¸º0ï¼‰
            reward_score = max(0.0, 1.0 - distance / 5.0)

            if reward_score > 0.0:
                dim_scale = self.target_vector.size(0) / 50  # 50 â†’ åŸå§‹åŸºå‡†
                dilution_factor = 1.0
                if self.current_step >= 5000:  # 5000æ­¥åå¼€å§‹å¥–åŠ±ç¨€é‡Š
                    dilution_factor = max(0.5, 1.0 - 0.00005 * (self.current_step - 5000))

                for unit in self.units:
                    if unit.get_role() == "processor":
                        unit.energy += 0.04 * dim_scale * reward_score * dilution_factor
                    elif unit.get_role() == "emitter":
                        unit.energy += 0.04 * dim_scale * reward_score * dilution_factor

                logger.debug(f"[å¥–åŠ±] è¾“å‡ºæ¥è¿‘ç›®æ ‡ï¼Œè·ç¦» {distance:.2f}ï¼Œå¥–åŠ±æ¯”ç‡ {reward_score:.2f} â†’ èƒ½é‡åˆ†é…å®Œæ¯•")

            # âœ… å¢åŠ å¤šæ ·æ€§æƒ©ç½šï¼ˆå«æ•°é‡åˆ¤æ–­ï¼‰
            action_indices = [torch.argmax(out).item() for out in outputs]

            if len(action_indices) >= 3:  # è‡³å°‘ 3 ä¸ª emitter æ‰æœ‰ç»Ÿè®¡æ„ä¹‰
                common_action = max(set(action_indices), key=action_indices.count)
                if action_indices.count(common_action) > len(action_indices) * 0.9:
                    for unit in self.units:
                        if unit.get_role() == "emitter":
                            unit.energy -= 0.05
                            logger.debug(f"[æƒ©ç½š] emitter {unit.id} å› è¾“å‡ºå•ä¸€è¡Œä¸ºè¢«æ‰£èƒ½é‡")
                elif len(set(action_indices)) > len(action_indices) * 0.6:
                    for unit in self.units:
                        if unit.get_role() == "emitter":
                            unit.energy += 0.01

                    logger.debug(f"[å¥–åŠ±] emitter è¾“å‡ºå¤šæ ·æ€§é«˜ â†’ æ‰€æœ‰ emitter +0.01 èƒ½é‡")

            else:
                logger.debug(f"[è·³è¿‡å¤šæ ·æ€§æƒ©ç½š] emitter æ•°é‡ä¸è¶³ï¼Œä»… {len(action_indices)} ä¸ª")

            if self.task.evaluate(self.env, outputs):
                if self.task.evaluate(self.env, outputs):
                    logger.debug(f"[ä»»åŠ¡å®Œæˆ] è¾¾æˆç›®æ ‡ä½ç½® {self.task.target_position}ï¼Œå¥–åŠ± +0.1")

                    for unit in self.units:
                        if unit.get_role() == "emitter":
                            unit.energy += 0.05  # æé«˜ emitter å¥–åŠ±
                        elif unit.get_role() == "processor":
                            unit.energy += 0.06  # ç»™ processor æ›´å¤šèƒ½é‡ï¼Œé¼“åŠ±å‚ä¸

        # === é‡åº¦ç»´æŠ¤ï¼šåªåœ¨éƒ¨åˆ†æ­¥æ•°æ‰§è¡Œï¼Œé¿å…æ¯æ­¥å¾ªç¯å¼€é”€ ===

        # â€”â€” å¯é€‰è·¯å¾„è¿½è¸ªï¼ˆçº¯è°ƒè¯•ï¼Œä¸å½±å“çŠ¶æ€ï¼‰ â€”â€”
        if self.debug and self.current_step % 50 == 0:
            self.trace_info_paths()

        # â€”â€” ç»Ÿä¸€ç»Ÿè®¡ & è¿æ¥æ‰“å°ï¼ˆä»… debugï¼‰ â€”â€”
        self._log_stats_and_conns()

        self.rebalance_cell_types()

        # === ğŸ” åˆ†è£‚ or å‚¨èƒ½ï¼šå¼ºåˆ¶å¤„ç†èƒ½é‡è¶…æ ‡å•å…ƒ ===
        while True:
            over_energy_units = [u for u in self.units if u.energy > 3.0]
            if not over_energy_units:
                break

            min_counts = self._get_min_target_counts()
            role_counts = Counter(u.get_role() for u in self.units if u.age < 240)

            for unit in over_energy_units:
                role = unit.get_role()
                if role_counts.get(role, 0) < min_counts[role] or self.total_energy() < self.max_total_energy:
                    # âœ… å½“å‰è§’è‰²æ•°é‡ä¸è¶³ æˆ– ç³»ç»Ÿèƒ½é‡æœªè¶…è½½ â†’ å¼ºåˆ¶åˆ†è£‚
                    expected_input = self.env_size * self.env_size * INPUT_CHANNELS
                    child = unit.clone(new_input_size=expected_input if unit.input_size != expected_input else None)
                    self.connect(unit, child)
                    self.auto_connect()
                    self.add_unit(child)
                    logger.info(f"[å¼ºåˆ¶åˆ†è£‚] {unit.id} ({role}) â†’ æ•°é‡ä¸è¶³/ç³»ç»Ÿæœªæ»¡ â†’ å¤åˆ¶")

                else:
                    # âš ï¸ ç³»ç»Ÿèƒ½é‡è¿‡è½½ & å½“å‰è§’è‰²æ•°é‡è¶³å¤Ÿ â†’ å‚¨èƒ½
                    contribution = unit.energy * 0.5
                    unit.energy *= 0.5
                    self.energy_pool += contribution
                    logger.debug(
                        f"[èƒ½é‡è½¬ç§»] {unit.id} ({role}) ç³»ç»Ÿè¿‡è½½ â†’ å­˜å…¥èƒ½é‡æ±  {contribution:.2f}ï¼Œä¿ç•™ {unit.energy:.2f}")

        # === 40 %-é™é¢å¤åˆ¶ï¼ˆ>15 ç»†èƒæ‰è§¦å‘ï¼‰ ===
        selected_parents = self._select_clone_parents(pending)
        for parent in selected_parents:
            expected_input = self.env_size * self.env_size * INPUT_CHANNELS

            child = parent.clone(
                new_input_size=expected_input if parent.input_size != expected_input else None
            )
            # çˆ¶å­è¿æ¥ï¼ˆå«ç»§æ‰¿ä¸Šä¸‹æ¸¸ï¼‰
            self.connect(parent, child)
            self.auto_connect()  # âœ… è®©æ–°å•å…ƒè‡ªè¡Œå¯»æ‰¾è¿æ¥å¯¹è±¡

            new_units.append(child)
            # â€”â€” æœ€ç»ˆä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ child åŠ å…¥å›¾ç»“æ„ â€”â€”
        for unit in new_units:
            self.add_unit(unit)

        # â€”â€” å®šæœŸåˆå¹¶ & é‡æ„ï¼ˆæ ¸å¿ƒç®—æ³•ï¼Œå¿…é¡»ä¿ç•™ï¼‰ â€”â€”
        if self.current_step % 100 == 0:
            self.merge_redundant_units()
            self.restructure_common_subgraphs()

        # === ğŸª« èƒ½é‡æ± è¡¥ç»™æœºåˆ¶ï¼šæ”¯æŒèƒ½é‡ä½çš„ç»†èƒ ===
        if self.energy_pool > 0.0:
            weak_units = [u for u in self.units if u.energy < 0.8]
            if weak_units:
                per_unit = min(0.2, self.energy_pool / len(weak_units))
                for u in weak_units:
                    u.energy += per_unit
                    self.energy_pool -= per_unit
                logger.info(f"[èƒ½é‡è¡¥ç»™] ä»èƒ½é‡æ± ä¸º {len(weak_units)} ä¸ªå¼±ç»†èƒè¡¥å…… {per_unit:.2f} èƒ½é‡")

    def upscale_old_units(self, new_input_size):
        """å°†æ‰€æœ‰ input_size å°äºå½“å‰ç¯å¢ƒé¢„æœŸå°ºå¯¸çš„å•å…ƒå‡ç»´ï¼ˆåªå‡ä¸é™ï¼‰"""
        for unit in self.units:
            if unit.input_size < new_input_size:
                logger.info(f"[å‡ç»´] {unit.id} input_size {unit.input_size} â†’ {new_input_size}")

                # 1. å‡ç»´ last_output
                old_output = unit.last_output
                if old_output.dim() == 2 and old_output.shape[0] == 1:
                    old_output = old_output.squeeze(0)
                padded_output = torch.zeros(new_input_size, device=old_output.device)
                padded_output[:old_output.shape[0]] = old_output
                unit.last_output = padded_output

                # 2. æ£€æŸ¥ hidden_size æ˜¯å¦ä¹Ÿè¦å‡é«˜ï¼ˆåªå‡ä¸é™ï¼‰
                if unit.state.shape[0] > unit.hidden_size:
                    unit.hidden_size = unit.state.shape[0]
                    logger.info(f"[å‡ç»´] {unit.id} hidden_size å‡è‡³ {unit.hidden_size}")

                # 3. å‡ç»´ state
                old_state = unit.state.squeeze(0) if unit.state.dim() == 2 else unit.state
                padded_state = torch.zeros(unit.hidden_size, device=old_state.device)
                length = min(padded_state.shape[0], old_state.shape[0])
                padded_state[:length] = old_state[:length]
                unit.state = padded_state

                # 4. é‡å»ºç½‘ç»œç»“æ„
                unit.function = torch.nn.Sequential(
                    torch.nn.Linear(new_input_size, unit.hidden_size),
                    torch.nn.ReLU(),
                    torch.nn.Linear(unit.hidden_size, new_input_size)
                )

                # 5. æ›´æ–° input_size
                unit.input_size = new_input_size

    def summary(self):
        # æ‰“å°å½“å‰å›¾ç»“æ„æ¦‚å†µ

        logger.debug(f"[å›¾ç»“æ„] å½“å‰å•å…ƒæ•°: {len(self.units)}")
        for unit in self.units:
            logger.debug(f" - {unit} â†’ è¿æ¥æ•°: {len(self.connections[unit.id])}")

    def collect_emitter_outputs(self):
        """æ”¶é›†æ‰€æœ‰ emitter è¾“å‡ºå¹¶è‡ªåŠ¨å¯¹é½åˆ°ç›®æ ‡ç»´åº¦"""
        aligned = []
        for unit in self.units:
            if unit.get_role() != "emitter":
                continue

            raw = unit.get_output().squeeze(0) if unit.get_output().dim() == 2 else unit.get_output()
            vec = self._align_to_goal_dim(raw)

            if vec.shape[-1] != self._goal_dim():
                # ç†è®ºä¸ä¼šå‘ç”Ÿï¼Œå®‰å…¨æ£€æŸ¥
                logger.warning(f"[è­¦å‘Š] å¯¹é½å¤±è´¥ {unit.id} é•¿åº¦ {vec.shape[-1]}")
                continue

            aligned.append(vec.unsqueeze(0))

        if aligned:
            stacked = torch.cat(aligned, dim=0)      # [N, goal_dim]
            logger.debug("[è¾“å‡ºæ£€æŸ¥] Emitter å¯¹é½åå‡å€¼(å‰5) :", stacked.mean(dim=0)[:5])
            return stacked
        else:
            logger.debug("[è¾“å‡ºæ£€æŸ¥] å½“å‰æ²¡æœ‰æ´»è·ƒçš„ emitter å•å…ƒ")
            return None



def interpret_emitter_output(output_tensor):
    """
    å°† emitter çš„è¾“å‡ºå‘é‡è§£é‡Šä¸ºåŠ¨ä½œã€‚
    """
    action_names = [f"åŠ¨ä½œ{i}" for i in range(50)]
    if output_tensor.dim() == 3:
        output_tensor = output_tensor.squeeze(1)  # å˜æˆ [N, 8]

    for i, out in enumerate(output_tensor):
        raw_index = torch.argmax(out).item()
        action_index = raw_index % 4  # ğŸŒŸ æŠ˜å åˆ° 0~3
        action = ["ä¸Š", "ä¸‹", "å·¦", "å³"][action_index]  # æˆ–è€…è‡ªå®šä¹‰åŠ¨ä½œåç§°
        logger.debug(f"[è¡Œä¸ºè§¦å‘] ç¬¬ {i + 1} ä¸ª emitter æ‰§è¡ŒåŠ¨ä½œ: {action}ï¼ˆåŸå§‹ index = {raw_index}ï¼‰")


def environment_feedback(output_tensor, graph):
    """
    ç¯å¢ƒå¯¹ emitter è¾“å‡ºçš„ç®€å•åé¦ˆæœºåˆ¶ï¼š
    - å¦‚æœè¾“å‡ºä¸­å‡ºç°ç‰¹å®šæ¨¡å¼ï¼ˆä¾‹å¦‚ â†‘ å’Œ â†’ åŒæ—¶è¾ƒå¼ºï¼‰ï¼Œå¥–åŠ±å¯¹åº” emitter
    - å¥–åŠ±é€šè¿‡æå‡ energy å®ç°
    """
    if output_tensor.dim() == 3:
        output_tensor = output_tensor.squeeze(1)  # [N, 8]

    for i, out in enumerate(output_tensor):
        # ç¤ºä¾‹æ¡ä»¶ï¼šè‹¥ â†‘ (index 0) å’Œ â†’ (index 3) è¾“å‡ºå€¼éƒ½å¤§äº 0.2
        if out[5] > 0.3 and out[13] < -0.1:  # è‡ªå®šä¹‰è§„åˆ™
            emitter = [u for u in graph.units if u.get_role() == "emitter"][i]
            emitter.energy += 0.05  # ç®€å•å¥–åŠ±
            logger.debug(f"[å¥–åŠ±] emitter {emitter.id} å›  â†‘+â†’ è¢«å¥–åŠ± +0.05 èƒ½é‡")



# if __name__ == "__main__":
#     print(logger.handlers)
#     env = GridEnvironment(size=5)
#     # åˆå§‹åŒ–ä»»åŠ¡ç›®æ ‡ï¼ˆä¾‹å¦‚ç›®æ ‡ä½ç½®åœ¨å³ä¸‹è§’ (4, 4)ï¼‰
#     task = TaskInjector(target_position=(4, 4))
#     goal_tensor = task.encode_goal(env.size)  # ç”Ÿæˆ 25ç»´ one-hot å‘é‡
#     graph = CogGraph()
#
#     # åˆå§‹åŒ–å•å…ƒ
#     sensor = CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS
# , role="sensor")
#     emitter = CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS
# , role="emitter")
#
#
#     # å¤šä¸ª processor
#     processor_list = []
#     for _ in range(4):
#         p = CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS
# , role="processor")
#         processor_list.append(p)
#
#
#
#     # åŠ å…¥å›¾ç»“æ„
#     graph.add_unit(sensor)
#     graph.add_unit(emitter)
#     for p in processor_list:
#         graph.add_unit(p)
#
#     # å»ºç«‹è¿æ¥
#     for p in processor_list:
#         graph.connect(sensor, p)
#         graph.connect(p, emitter)
#
#     # --- æ–°å¢ï¼šé¢å¤–ç§å­ ---
#     extra_sensors = [CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS,
#                              role="sensor") for _ in range(1)]  # å†è¡¥ 1 ä¸ª
#     extra_emitters = [CogUnit(input_size=graph.env_size * graph.env_size * INPUT_CHANNELS,
#                               role="emitter") for _ in range(1)]  # å†è¡¥ 1 ä¸ª
#     for u in extra_sensors + extra_emitters:
#         graph.add_unit(u)
#         # è®©æ¯ä¸ª sensor â†’ processor[0]ï¼Œprocessor[-1] â†’ æ¯ä¸ª emitterï¼Œä¿è¯ä¿¡æ¯é€šè·¯
#         graph.connect(u, processor_list[0]) if u.role == "sensor" else graph.connect(processor_list[-1], u)
#
#     # è¿è¡Œæ¨¡æ‹Ÿ
#     for step in range(20000):
#         logger.info(f"\n==== ç¬¬ {step+1} æ­¥ ====")
#         # 1ï¸âƒ£ è·å–ç¯å¢ƒçŠ¶æ€ï¼Œè¾“å…¥ sensor
#         state = env.get_state()
#         input_tensor = torch.cat([
#             torch.from_numpy(state).float(),
#             goal_tensor
#         ], dim=0)
#
#         # 2ï¸âƒ£ å¯åŠ¨è®¤çŸ¥ç³»ç»Ÿï¼ˆæ„ŸçŸ¥ + å†³ç­–ï¼‰
#         graph.step(input_tensor)
#
#         # 3ï¸âƒ£ æ”¶é›† emitter è¾“å‡ºï¼Œè½¬æ¢ä¸ºåŠ¨ä½œ
#         emitter_output = graph.collect_emitter_outputs()
#         if emitter_output is not None:
#             raw_idx = torch.argmax(emitter_output.mean(dim=0)).item()
#             action_index = raw_idx % 4  # æŠ˜å åˆ° 0-3
#             logger.debug(f"[è¡ŒåŠ¨å†³ç­–] æ‰§è¡ŒåŠ¨ä½œ: {action_index}")
#
#             # 4ï¸âƒ£ æ‰§è¡ŒåŠ¨ä½œï¼Œæ”¹å˜ç¯å¢ƒ
#             env.step(action_index)
#             # ğŸ” å°†ç¯å¢ƒèƒ½é‡å˜åŒ–åé¦ˆç»™ emitter
#             for unit in graph.units:
#                 if unit.get_role() == "emitter":
#                     unit.energy += env.agent_energy_gain
#                     unit.energy -= env.agent_energy_penalty
#                     logger.debug(f"[ç¯å¢ƒåé¦ˆ] {unit.id} +{env.agent_energy_gain:.2f} -{env.agent_energy_penalty:.2f}")
#
#         # 5ï¸âƒ£ æ‰“å°ç¯å¢ƒå›¾ç¤º
#         env.render()
#
#         # å¦‚æœæ²¡æœ‰å•å…ƒå‰©ä¸‹ï¼Œé€€å‡ºd
#         if not graph.units:
#             logger.info("[ç»ˆæ­¢] æ‰€æœ‰å•å…ƒæ­»äº¡ã€‚")
#             break
#
# from collections import Counter
# final_counts = Counter([unit.get_role() for unit in graph.units])
# print("\nğŸ§¬ æœ€ç»ˆç»†èƒæ€»æ•°ç»Ÿè®¡ï¼š", dict(final_counts))
# print("ğŸ”¢ æ€»ç»†èƒæ•° =", len(graph.units))
# print(f"\nğŸ§ª æ¨¡æ‹Ÿç»“æŸåèƒ½é‡æ± å‰©ä½™ï¼š{graph.energy_pool:.2f}")
#
