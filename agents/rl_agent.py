"""
RLAgent
=======
è½»é‡çº§ç­–ç•¥æ¢¯åº¦ä»£ç†ï¼ˆé»˜è®¤ REINFORCEï¼‰ã€‚

âœ¦ åŠŸèƒ½
1. åŒ…è£… TransformerPolicyNetworkï¼›
2. è´Ÿè´£ action é‡‡æ ·ã€è½¨è¿¹ç¼“å­˜ï¼›
3. åœ¨ episode ç»“æŸåæ‰§è¡Œç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼›
4. æä¾› save / load æ¥å£æ–¹ä¾¿æ–­ç‚¹ç»­è®­ã€‚

âœ¦ ä¾èµ–
- models.transformer_policy.TransformerPolicyNetwork
"""

from __future__ import annotations

import torch
import torch.nn as nn
from torch.distributions import Categorical
from typing import List

from models.transformer_policy import TransformerPolicyNetwork



class RLAgent:
    """æ— å€¼å‡½æ•°çš„çº¯ç­–ç•¥ä»£ç†ï¼ˆREINFORCEï¼‰ã€‚"""

    def __init__(
        self,
        input_dim: int,
        num_actions: int,
        lr: float = 3e-4,
        gamma: float = 0.99,
        d_model: int = 64,
        device: str | torch.device = "cpu",
    ) -> None:
        self.device = torch.device(device)
        # 1) å…ˆæ„é€ ç­–ç•¥ç½‘
        self.policy_net = TransformerPolicyNetwork(
            input_dim=input_dim,
            num_actions=num_actions,
            d_model=d_model  # â† çœŸæ­£ç”¨åˆ°ä¼ å…¥çš„ d_model
        ).to(self.device)

        # 2) å†æ„é€ å€¼å‡½æ•°
        self.value_head = nn.Sequential(
            nn.Linear(input_dim, 1)
        ).to(self.device)



        # 3) ä¼˜åŒ–å™¨æŠŠä¸¤éƒ¨åˆ†å‚æ•°éƒ½åŠ è¿›æ¥
        params = list(self.policy_net.parameters()) + list(self.value_head.parameters())
        self.optimizer = torch.optim.Adam(params, lr=lr)

        self.gamma = gamma  # â† ä¿å­˜æŠ˜æ‰£å› å­

        # â€”â€” è½¨è¿¹ç¼“å­˜ â€”â€”
        self.log_probs: List[torch.Tensor] = []
        self.rewards:   List[float]       = []
        self.saved_states = []  # baseline éœ€è¦çŠ¶æ€è¾“å…¥


    # --------------------------------------------------------------------- #
    #                           äº¤äº’æ¥å£                                     #
    # --------------------------------------------------------------------- #

    def expand_value_head(self, new_input_dim):
        old_layer = self.value_head[0]  # å– Sequential ä¸­çš„ Linear å±‚
        old_input_dim = old_layer.in_features

        if new_input_dim <= old_input_dim:
            return  # ä¸éœ€è¦æ‰©å±•

        # æ„å»ºæ–° Linear å±‚ï¼ˆä¿ç•™æ—§å‚æ•°ï¼‰
        new_layer = torch.nn.Linear(new_input_dim, 1).to(old_layer.weight.device)

        with torch.no_grad():
            # æ‹·è´æ—§æƒé‡ï¼ˆåªæ‹·è´å‰ old_input_dim éƒ¨åˆ†ï¼‰
            new_layer.weight[:, :old_input_dim] = old_layer.weight
            new_layer.bias = old_layer.bias

        self.value_head = torch.nn.Sequential(new_layer)
        print(f"[ğŸ” å‡ç»´] value_head è¾“å…¥ç»´åº¦ {old_input_dim} â†’ {new_input_dim}")

    def select_action(self, state_seq: torch.Tensor) -> int:
        """
        ç»™å®šçŠ¶æ€åºåˆ—ï¼Œé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œã€‚

        Args:
            state_seq: Tensor, shape=(1, seq_len, input_dim)

        Returns:
            int: åŠ¨ä½œç´¢å¼•
        """
        state_seq = state_seq.to(self.device)
        logits = self.policy_net(state_seq)          # (1, num_actions)
        dist = Categorical(logits=logits)
        action = dist.sample()                       # Tensor([])
        self.log_probs.append(dist.log_prob(action)) # ç¼“å­˜æœ¬æ­¥ log Ï€(a|s)
        state_feat = state_seq.detach().mean(dim=1)  # (1, input_dim)
        self.saved_states.append(state_feat)  # â† ä¿å­˜ä¸€ä»½çŠ¶æ€åºåˆ—ï¼Œç”¨äº baseline å€¼å‡½æ•°
        return action.item()

    def store_reward(self, r: float) -> None:
        """åœ¨ç¯å¢ƒæ­¥ç»“æŸåè°ƒç”¨ï¼Œç¼“å­˜å³æ—¶å›æŠ¥ã€‚"""
        self.rewards.append(r)

    # --------------------------------------------------------------------- #
    #                           å­¦ä¹ æ›´æ–°                                     #
    # --------------------------------------------------------------------- #

    def _compute_returns(self) -> torch.Tensor:
        """
        æŠ˜æ‰£å›æŠ¥ G_t = Î£ Î³^k r_{t+k}

        Returns:
            Tensor, shape=(T,)
        """
        R = 0.0
        returns = []
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)
        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)
        # å½’ä¸€åŒ–ï¼Œæå‡æ•°å€¼ç¨³å®šæ€§
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        return returns

    def finish_episode(self):
        R = 0
        returns = []
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)
        returns = torch.tensor(returns, device=self.device)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # æ–° Actor-Critic æŸå¤±
        policy_loss = []
        value_loss = []

        for log_prob, state_feat, R in zip(self.log_probs, self.saved_states, returns):
            self.expand_value_head(state_feat.shape[-1])  # ç¡®ä¿ value_head æ”¯æŒå½“å‰è¾“å…¥ç»´åº¦
            value = self.value_head(state_feat).squeeze()
            advantage = R - value.detach()
            policy_loss.append(-log_prob * advantage)
            value_loss.append(torch.nn.functional.mse_loss(value, R))

        loss = torch.stack(policy_loss).sum() + 0.5 * torch.stack(value_loss).sum()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.log_probs.clear()
        self.rewards.clear()
        self.saved_states.clear()

    # --------------------------------------------------------------------- #
    #                          æ¨¡å‹æŒä¹…åŒ–                                    #
    # --------------------------------------------------------------------- #

    def save(self, path: str) -> None:
        torch.save({
            "policy_state_dict": self.policy_net.state_dict(),
            "value_state_dict": self.value_head.state_dict(),  # â† æ–°å¢
            "optimizer_state_dict": self.optimizer.state_dict(),
        }, path)

    def load(self, path: str, map_location: str | torch.device | None = None) -> None:
        checkpoint = torch.load(path, map_location=map_location or self.device)
        self.policy_net.load_state_dict(checkpoint["policy_state_dict"])
        self.value_head.load_state_dict(checkpoint["value_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])


"""
è¯´æ˜è¦ç‚¹

æ¨¡å—	             ä½œç”¨
select_action	å‰å‘æ¨æ–­ â†’ Categorical â†’ é‡‡æ · â†’ è®°å½• log_prob
store_reward	åœ¨æ¯ä¸ª env.step() åè°ƒç”¨ï¼Œç¼“å­˜å³æ—¶å¥–åŠ±
finish_episode	è®¡ç®—æŠ˜æ‰£å›æŠ¥ â†’ æ ‡å‡†åŒ– â†’ REINFORCE æ›´æ–°
save / load	ä¾¿äºé•¿æ—¶é—´è®­ç»ƒä¸­æ–­ç‚¹ç»­è®­

å¦‚æœåç»­æ”¹ç”¨ PPO / A2C ç­‰ç®—æ³•ï¼Œåªéœ€ï¼š

æ›¿æ¢ç­–ç•¥æ›´æ–°éƒ¨åˆ†ï¼›

å¢åŠ å€¼å‡½æ•°æˆ–æ—§ç­–ç•¥ç¼“å­˜ã€‚å…¶ä»–æ¥å£ä¿æŒä¸å˜ã€‚
"""